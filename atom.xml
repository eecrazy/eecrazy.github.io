<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>李忠阳的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-11-06T05:24:38.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhongyang Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>事理图谱标注</title>
    <link href="http://yoursite.com/2016/11/06/%E4%BA%8B%E7%90%86%E5%9B%BE%E8%B0%B1%E6%A0%87%E6%B3%A8/"/>
    <id>http://yoursite.com/2016/11/06/事理图谱标注/</id>
    <published>2016-11-06T04:29:07.000Z</published>
    <updated>2016-11-06T05:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="需要修改的例子"><a href="#需要修改的例子" class="headerlink" title="需要修改的例子"></a>需要修改的例子</h1><p>台阶上堆满蜡烛和乞讨的人</p>
<p>累的半死<br>一起走<br>喘气</p>
<p>待得心烦<br>先走</p>
<p>看到游客经过</p>
<p>目光交流<br>下山<br>坐缆车</p>
<p>挥泪告别<br>自己攀登</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;需要修改的例子&quot;&gt;&lt;a href=&quot;#需要修改的例子&quot; class=&quot;headerlink&quot; title=&quot;需要修改的例子&quot;&gt;&lt;/a&gt;需要修改的例子&lt;/h1&gt;&lt;p&gt;台阶上堆满蜡烛和乞讨的人&lt;/p&gt;
&lt;p&gt;累的半死&lt;br&gt;一起走&lt;br&gt;喘气&lt;/p&gt;
&lt;p&gt;待得心烦&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2016/11/06/hello-world/"/>
    <id>http://yoursite.com/2016/11/06/hello-world/</id>
    <published>2016-11-06T02:50:38.000Z</published>
    <updated>2016-11-06T04:25:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h2 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h2 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h2 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h2 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>事理图谱思考</title>
    <link href="http://yoursite.com/2016/04/24/%E4%BA%8B%E7%90%86%E5%9B%BE%E8%B0%B1%E6%80%9D%E8%80%83-2016-04-24/"/>
    <id>http://yoursite.com/2016/04/24/事理图谱思考-2016-04-24/</id>
    <published>2016-04-24T09:28:05.000Z</published>
    <updated>2016-11-06T04:34:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a>研究意义</h1><p>事理图谱：以事件为元素的常识推理知识库<br>Open Information Extraction：Event Extraction<br>基于特定场景的事件链条：意义何在？<br>TDT：Topic Detection and Tracking</p>
<p>现在的第一要务：<br>手工筛选500条问答对语料，400条训练，100条测试。<br>手工标注每个问答对可以构建的事件链条。<br>按照嘉伟的规则，以及我的问题的特殊性，完成完善的事件抽取模块。提高准确率和召回率。</p>
<p>事件抽取的规则制定：<br>如果主语是人或者没有主语，则可以不要主语，否则的话应该加上主语 SBV<br>动词的修饰副词应该加上,如不，没等<br>并列结构应该要：COO</p>
<p>事件最小单元：动宾短语-如果主语是人或者没有主语，则可以不要主语，否则的话应该加上主语 SBV</p>
<p>段落分类，一个问答对是否可以满足构建</p>
<p>动作动词和状态动词词表</p>
<p>后续工作计划：<br>web端demo展示：出行类消费事理图谱<br>事理图谱的存储和查询：事件抽取然后进行相似度计算，找到最相似节点。为了加快相似度计算速度，可以做个事件聚类。<br>开放域大规模语料上构建事理图谱，用于对话系统和意图识别。</p>
<p>先基于特定领域来做：股票涨跌的事理图谱-&gt;方法迁移-&gt;出行意图的事理图谱</p>
<p>技术方案and实验：<br>1.找到句子；<br>2.识别句子之间的关系，即箭头方向；</p>
<p>以上两步构建了单个句对的事理例子，下面进一步构建成为事理图谱：</p>
<p>3.将句子进行抽象，去除无用成分；同一个句子中可能包含多个事件，应识别同一个句子中的事件之间的关系，和不同句子之间的关系。</p>
<p>4.将成对的句子根据推理关系进行拼接，构建事件链条；<br>5.进行句子相似度计算，同一意思的句子进行规约，形成图谱形态；<br>6.领域迁移。</p>
<p>不足之处：<br>1.方法可迁移吗？<br>2.人工参与多吗？<br>3.信息过滤自动化吗？如何高效判断句子的信息含量？或者是否含有因果关系？<br>4.需要规则吗？规则可以领域迁移吗？<br>5.如果用到了关键字匹配，如何为不同领域自动选择关键字？</p>
<p>工作意义：<br>1.自动构建事理图谱：事件背后的道理<br>2.领域迁移<br>3.基于事件的预测可解释性更好</p>
<p>相关工作：<br>1.事件抽取；<br>2.因果关系挖掘；<br>3.基于事件的预测；<br>4.事件的表示；<br>5.知识图谱；</p>
<p>Title: Constructing Event Graph For (implicit) Consumption Intention Recognition (in dialogue system)</p>
<p>Mining Consumption Intention From Online Q&amp;A Community   </p>
<p>Author:<br>Zhongyang Li, Xiao Ding, and Ting Liu<br>Reseach Center for Social Computing and Information Retrieval,<br>Harbin Institute of Technology, Harbin, China<br>{zyli,xding,tliu}@ir.hit.edu.cn</p>
<p>KeyWords:<br>事理图谱（事件网络），消费意图识别，网络构建，意图相关产品推荐，知乎，问答社区</p>
<p>Abstract:</p>
<p>Introduction:<br>the importance of Intention Recognition especially the Recognition of Consumption Intention.<br>two main kinds of Consumption Intention: explicit and implicit.<br>other Intention related work and related work.<br>contributions of this work:<br>    1.提出事理图谱的概念。<br>    2.用知乎数据构建出行类消费事理图谱：如何描述这个网络，节点和边如何定义。<br>    3.事件相关消费意图识别：搜索引擎，电商，淘宝，京东。</p>
<p>Problem definition:<br>显式消费意图，隐式消费意图，事理图谱定义，消费意图跟事件紧密相关。<br>构建消费事理图谱，挖掘每一事件相关的消费行为。<br>    存在的问题：<br>如何进行事件抽取？<br>如何构建事件链条？<br>如何挖掘事件相关的消费意图？<br>如何处理同一事件的不同描述？<br>如何处理事件抽取造成的事件歧义？<br>如何描述每一个意图？只给出产品类别？<br>如何构建baseline？<br>如何评价？</p>
<p>Methods:</p>
<p>part1:<br>part2:</p>
<p>Experiments:<br>dataset:<br>    知乎旅行数据集<br>    数据集上的统计:问题数，答案数，平均每个问题的回答数，平均答案长度<br>experiments setup:<br>    baseline methods：<br>    事件抽取<br>    事件相关的消费意图识别<br>evaluation metrics:<br>    准确率，召回率<br>results and analisys:<br>    抽取了多少个事件<br>    其中多少个事件是消费意图相关的<br>    事件链条的评价：多少个一阶，二阶，三阶，更多阶事件链条。图谱评价：定点数，边数，度数等（寻找评价图的完备性的指标）</p>
<p>Realted work:</p>
<p>Conclusion and Future Work:<br>我们论述了：意图识别尤其是消费意图识别具有极大的商业价值。传统基于分类的方法对于消费意图识别任务作用有限，尤其是难以准确识别隐式消费意图.<br>本文提出了事理图谱的概念，并针对于消费意图识别任务，提出并构建了消费事理图谱，进一步挖掘了事件相关的消费意图。<br>本文提出了解决消费意图识别的新方法，对于不同场景下的隐式消费意图识别具有重要意义。</p>
<p>future work:<br>用更大的数据集构建具有实用意义的消费事理图谱，扩展到其他领域，探究通用消费事理图谱的构建方法，在实际商业场景中检验事理图谱的有效性。<br>Acknowledgement:</p>
<p>References:</p>
<p>从事专业关键词:顺序跟word文档不一致</p>
<p>平台团队：删了一段最后一段</p>
<p>第五篇论文word文档中是代表作</p>
<p>错误原因：<br>句子不规则，缺少标点，如果加上标点，就可以句法分析正确。<br>分词，词性标注，句法分析错误。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;研究意义&quot;&gt;&lt;a href=&quot;#研究意义&quot; class=&quot;headerlink&quot; title=&quot;研究意义&quot;&gt;&lt;/a&gt;研究意义&lt;/h1&gt;&lt;p&gt;事理图谱：以事件为元素的常识推理知识库&lt;br&gt;Open Information Extraction：Event Extra
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="研究" scheme="http://yoursite.com/tags/%E7%A0%94%E7%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>网页-微信文章-科普文章阅读笔记</title>
    <link href="http://yoursite.com/2016/04/02/%E7%BD%91%E9%A1%B5-%E5%BE%AE%E4%BF%A1%E6%96%87%E7%AB%A0-%E7%A7%91%E6%99%AE%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-2016-04-02/"/>
    <id>http://yoursite.com/2016/04/02/网页-微信文章-科普文章阅读笔记-2016-04-02/</id>
    <published>2016-04-02T14:20:41.000Z</published>
    <updated>2016-11-06T04:34:40.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li><p>机器学习那些事，刘知远<br>得到的经验就是：<br>特征工程很重要：特征选择，试错<br>训练多个不同模型集成很重要：bagging，boosting，stacking</p>
</li>
<li><p>深度学习：推动NLP领域发展的新引擎</p>
</li>
</ol>
<p>Word Embedding<br>引入词的关系<br>最常见的思路就是用Dependency Parser，把抽取出来的Relation作为词的Context。<br>改进Bag of Words<br>有人认为词（Word）的粒度也太大，可以到Character级别的，或者Morpheme级别的。<br>外部资源和知识库<br>Word2vec只使用了词的上下文的共现，没有使用外部的资源如词典知识库等，因此也有不少工作对此进行改进。</p>
<p>RNN/LSTM/CNN<br>最近CNN相关的改进模型也被用于NLP领域。今年的ACL上有很多RNN/LSTM/CNN用来做机器翻译（Machine Translation）、语义角色标注（Sematic Role Labeling）等。</p>
<p>Multi-model Deep Learning<br>这是当下的一个热门，不只考虑文本，同时也考虑图像，比如给图片生成标题（Caption）。当然这和传统的NLP任务不太一样，但这是一个非常有趣的方向，有点像小朋友学习看图说话。</p>
<p>Reasoning, Attention and Memory<br>前面说RNN/LSTM是试图模拟人类大脑的记忆机制，但除了记忆之外，Attention也是非常有用的机制。</p>
<p>Attention<br>最早Attention是在《Recurrent Models of Visual Attention》这篇文章提出来的。关于Attention，Google DeepMind的一篇论文《DRAW：A Recurrent Neural Network For Image》有一段非常好的解释。Attention除了模仿人类视觉系统的认知过程，还可以减少计算量，因为它排除了不关心的内容。而传统的模型如CNN，其计算复杂度就非常高。另外除了计算减少的好处之外，有选择地把计算资源（注意力）放在关键的地方而不是其它（可能干扰）的地方，还有可能提高识别准确率。就像一个人注意力很分散，哪都想看，反而哪都看不清楚。</p>
<p>Attention最早是在视觉领域应用，当然很快就扩展到NLP和Speech。<br>用来做机器翻译：Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. 2015. In Proceedings of ICLR.</p>
<p>做Summary：Alexander M. Rush, Sumit Chopra, Jason Weston. A Neural Attention Model for Sentence Summarization. 2015. In Proceedings of EMNLP.</p>
<p>Word Embedding: Not All Contexts Are Created Equal: Better Word Representations with Variable Attention. 2015. In Proceedings of EMNLP.</p>
<p>Speech领域：Attention-Based Models for Speech Recognition. </p>
<p>其它的应用，比如Multimodel，Image的Caption生成： Attend and Tell: Neural Image Caption Generation with Visual Attention. Teaching Machines to Read and Comprehend. 2015. In Proceedings of NIPS.</p>
<p>前面最早的Attention Model是不可导的，只能用强化学习来优化，也被叫做Hard Attention，也就是把注意力集中在离散的区域；后来也有Soft的Attention，也就是在所有的区域都有Attention，但是连续分布的。Soft的好处是可导，因此可以用梯度下降这样的方法来训练模型，和传统的神经网络更加接近。但坏处就是因为所有的区域都有值（非零），这就增加了计算量。用个不恰当的比方，一个是稀疏矩阵，一个是稠密的矩阵，计算量当然差别就很大。</p>
<p>也有一些工作尝试融合Soft和Hard Attention的优点。</p>
<p>Memory的扩展<br>前面说到RNN，如LSTM，有Memory（记忆），很多模型对此也进行了拓展。</p>
<p>比如Neural Turing Machine (Neural Turing Machines. Alex Graves, Greg Wayne, Ivo Danihelka. arXiv Pre-Print, 2014)，NTM用一句话描述就是有外部存储的神经网络。</p>
<p>Language的复杂性<br>深度学习的一个方向Representation Learning其实就是有这个想法，不过目前更多关注的是一些具体任务的Feature的表示。更多是在Image和Speech领域，用在Language的较少，Word2vec等也可以看成表示概念的方式，不过这种向量的表示太过简单且没有结构化。更少有工作考虑用神经网络怎么表示人类已有的复杂知识。现在的知识表示还是以几十年前基于符号的形式逻辑的为主。</p>
<p>我们现在甚至有很多结构化的数据，比如企业数据库、维基百科的、Google的Freebase以及内部的Knowledge Graph。但目前都是用人类习惯的表示方式，比如三元组、图或者实体关系。但这样的表示方式是高层的抽象的，大脑里的神经元似乎不能处理，因此现在的Deep Neural Network很难整合已有的这些知识库进行推理等更有用的事情。</p>
<p>总结<br>从上面的分析我们大致可以看到最近NLP的发展趋势：深度神经网络尤其是RNN的改进，模拟人脑的Attention和Memory，更加结构化的Word Embedding或者说Knowledge Representation。</p>
<ol>
<li><p>sed百度百科阅读学习<br>sed -n ‘s/ID/&amp;id/p’ true.csv |more<br>sed /100/d true.csv |wc -l</p>
</li>
<li><p>如何判断一篇论文是否被SCI收录？<br>有人说IDS number就是SCI检索号，而也有很多人说SCI检索号是UT ISI号(15位)。不知谁的说法对？认为是后一种的人更多。<br>IDS 在sci数据库中是这样的解释的：Thomson Reuters Document Solution® 编号。此号码是识别期刊和期号的唯一编号，用于订阅 Document Solution 中的文献的全文。也就是说该号是论文发表刊和所在期的编号。由此可知同一期的所有的论文IDS号是一样的。UT ISI号是Unique Article Identifier，是文章的唯一识别符。<br>规律：<br>看ids号<br>数字在前  字母在后 SCI<br>字母在前  数字在后  ISTP</p>
</li>
</ol>
<p>“SCI、EI和ISTP收录号的查询方法（总结）”：<br><a href="http://emuch.net/bbs/viewthread.php?tid=660421" target="_blank" rel="external">http://emuch.net/bbs/viewthread.php?tid=660421</a><br>“GA  BHE43 ”这栏里的“BHE43”字母在前数字在后就是ISTP检索，如果数字在前字母在后就是SCI检索。当然要注意一个特殊的情况，有些会议文章被期刊收录后，即使该期刊是SCI检索，查询检索时“GA  BHE43 ”这栏里的“BHE43”情形还是和ISTP检索类似，即字母在前数字在后。</p>
<ol>
<li>原问题：如何查询一篇文献的影响因子<br>亲。文献是没有影响因子的。有影响因子的那个是期刊。你直接查到期刊的主页，上面会有影响因子的。或者期刊名加上impact factor直接上Google上搜就能搜到。</li>
</ol>
<p>影响因子（Impact Factor,IF)是美国ISI（科学信息研究所）的JCR(期刊引证报告）中的一项数据。<br>  即某期刊前两年发表的论文在统计当年的被引用总次数除以该期刊在前两年内发表的论文总数。这是一个国际上通行的期刊评价指标。<br>意义：该指标是相对统计值，可克服大小期刊由于载文量不同所带来的偏差。一般来说，影响因子越大，其学术影响力也越大。<br>影响因子查询系统<br><a href="http://emuch.net/sciif/" target="_blank" rel="external">http://emuch.net/sciif/</a><br><a href="http://www.proteomics.com.cn/sci-if/" target="_blank" rel="external">http://www.proteomics.com.cn/sci-if/</a></p>
<ol>
<li>Michael Jordan推荐的机器学习书籍<br>“I now tend to add some books that dig still further into foundational topics. In particular, I recommend A. Tsybakov’s book “Introduction to Nonparametric Estimation” as a very readable source for the tools for obtaining lower bounds on estimators, and Y. Nesterov’s very readable “Introductory Lectures on Convex Optimization” as a way to start to understand lower bounds in optimization. I also recommend A. van der Vaart’s “Asymptotic Statistics”, a book that we often teach from at Berkeley, as a book that shows how many ideas in inference (M estimation—which includes maximum likelihood and empirical risk minimization—the bootstrap, semiparametrics, etc) repose on top of empirical process theory. I’d also include B. Efron’s “Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction”, as a thought-provoking book”<br>Introduction to Nonparametric Estimation<br>Introductory Lectures on Convex Optimization<br>Asymptotic Statistics<br>Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction</li>
</ol>
<p>Tsybakov’s book is available online at Springer if your University has access to it: <a href="http://link.springer.com/book/10.1007%2Fb13794" target="_blank" rel="external">http://link.springer.com/book/10.1007%2Fb13794</a></p>
<p>Nesterov’s books is also available: <a href="http://link.springer.com/book/10.1007%2F978-1-4419-8853-9" target="_blank" rel="external">http://link.springer.com/book/10.1007%2F978-1-4419-8853-9</a></p>
<p>Thanks a lot! BTW, I gathered your recommendations on Goodreads: <a href="https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list" target="_blank" rel="external">https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list</a></p>
<p>In this post, I would like to blend together recommendations from academic and industry researchers: <a href="http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html" target="_blank" rel="external">http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html</a></p>
<p>Other comprehensive list of video lectures and some books. <a href="http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html" target="_blank" rel="external">http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html</a></p>
<ol>
<li><p>SCI论文发表过程分析<br>论文写作<br>论文定位<br>选定期刊<br>在线投稿<br>文章审理<br>文章接收</p>
</li>
<li><p>搜索需要一场变革<br>在万维网诞生20周年之际,奥伦·艾齐厄尼(OrenEtzioni)号召研究者开始思考比关键 词框更好的互联网搜索方式。<br>互联网搜索正处于从简单的文档检索走向问题 回答深刻变革的风口浪尖上。用户在进行搜索时真正想要的并<br>不是一串长长的包含关键词的文档列 表,而是他们所提问题的直接答案。</p>
</li>
</ol>
<p>由网页 构成的海量语料库是高度冗余的,也就是说一个断 言会以不同形式被多次表达。这样一来,当系统从 独立撰写的不同句子中多次抽取出相同的断言时, 它以此推导出一个正确事实的可能性便得到了成倍的提高。</p>
<p>开放式信息抽取或许能够从实质上扩大这些工 具的使用范围。我们系统的开源代码可以通过http:// go.nature.com/ei3p4f获取。</p>
<p>信息抽取不仅需要能够推 导由动词表达的关系,还要能够推导由名词或形容 词表达的关系。同时,信息往往要通过其<br>来源、目 的和上下文才能充分体现,因此系统需要建立检测 这些因素或者其他更微妙因素的能力。最后,信息 抽取方法<br>还需要扩展到其他语言中去,这也将面临 各种特殊的挑战。</p>
<p>实现从信息检索到问答系统的范式转变所遇到的 最大障碍似乎是我们令人惊异地缺乏某种雄心和想象力。</p>
<p>大多数自然语言处理研究聚焦于有限的几个任务 上。例如,它们关注如何分析句子的句法结构,却很 少关注如何揭示<br>它们的意义;由于对人工标注数据的依赖,其方法无法适应大规模文档集合和任意主题的文本;所提出算法的计算<br>花销也可能是随着数据的规模呈指数增长的。</p>
<p>无论是检索文献的科学家,还是迅速增长的小屏幕手机用户(需要从互联网中撷取简洁回答),均将 受惠于通用<br>的问答系统。没有它,我们将会冒被淹没 于信息汪洋大海之中的危险。</p>
<ol>
<li><p>特征工程方法<br>特征提取<br>特征构造<br>特征工程</p>
</li>
<li><p>kaggle特点</p>
</li>
<li><p>计算机科学的发展趋势，刘知远</p>
</li>
<li><p>复旦大学，GDM团队，肖仰化，崔万云（博三），图数据管理研究室</p>
</li>
</ol>
<p>页面上有很多十分有用的信息，学生都很厉害！！<br>verb pattern demo，AAAI2016，崔万云</p>
<ol>
<li>senticNet 网页阅读-信息丰富</li>
</ol>
<p>Sentic Team-Projects:</p>
<p>KNOWLEDGE REPRESENTATION<br>SUBJECTIVITY DETECTION<br>MULTIMODAL SENTIMENT ANALYSIS<br>MULTILINGUAL SENTIMENT ANALYSIS<br>AUTOMATIC SPEECH RECOGNITION<br>STOCK MARKET PREDICTION</p>
<ol>
<li>了解各个大型知识库的情况</li>
</ol>
<p>15.降低知识图谱的构造成本 文因互联 鲍捷</p>
<p>轻重型知识项目随着成本提升，回报的提升</p>
<p>降低成本：<br>    依托成熟技术：知识表示，提取，存储，检索，人机交互（每个阶段都涉及到一定的成本）<br>    迭代构建：data-idea-code</p>
<p>降低成本的核心是人：知识是写给人读的,只是碰巧能被机器执行</p>
<ol>
<li>李飞飞做研究的建议<br>important problem (inspiring idea) + solid and novel theory + convincing and analytical experiments + good writing = seminal research + excellent paper.<br>重要的问题 + 坚固而新颖的理论 + 令人信服的分析实验 + 一流的写作 = 开创性的研究 + 出色的论文</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;&lt;p&gt;机器学习那些事，刘知远&lt;br&gt;得到的经验就是：&lt;br&gt;特征工程很重要：特征选择，试错&lt;br&gt;训练多个不同模型集成很重要：bagging，boosting，stacking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;深度学习：推动NLP领域发展的新引擎&lt;/p&gt;
&lt;/
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记</title>
    <link href="http://yoursite.com/2016/01/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-2016-01-14/"/>
    <id>http://yoursite.com/2016/01/14/论文阅读笔记-2016-01-14/</id>
    <published>2016-01-14T02:08:19.000Z</published>
    <updated>2016-11-06T04:34:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习相关论文"><a href="#深度学习相关论文" class="headerlink" title="深度学习相关论文"></a>深度学习相关论文</h1><ol>
<li>机器学习系统，隐藏多少技术债？Hidden Technical Debt in Machine Learning Systems</li>
<li>Mastering the game of Go with deep neural networks and tree search_nature</li>
<li>Human-level control through deep reinforcement</li>
<li>Teaching Machines to Read and Comprehend</li>
</ol>
<p>#其他论文</p>
<ol>
<li>Personalized Microtopic Recommendation on Microblogs,李洋，TIST</li>
<li>GraphX: Graph Processing in a Distributed Dataflow Framework</li>
<li>An Experimental Comparison of Pregel-like Graph Processing Systems</li>
</ol>
<h1 id="事理图谱-EventNet-调研工作："><a href="#事理图谱-EventNet-调研工作：" class="headerlink" title="事理图谱(EventNet)调研工作："></a>事理图谱(EventNet)调研工作：</h1><p>Title : Another Commonsense Knowledge Base: EventNet<br>构建事理图谱需要解决的问题：<br>如何精确定义事件顺承关系，事件因果关系？<br>如何检测两个事件之间是否有关系？如果有关系，是属于顺承还是属于因果？<br>如何判断多个事件之间的顺承，因果依赖次序？<br>事理图谱的应用</p>
<ol>
<li>Diamonds in the Rough: Event Extraction from Imperfect Microblog Data  2015 NAACL</li>
</ol>
<p>地震事件抽取，CRF序列标注模型，20个事件参数<br>为了克服微博数据的不准确和短文本造成的模棱两可特点(会给评估和系统开发造成困难），使用了两个方法:approximate(模糊匹配) distant supervision和feature aggregation（聚合多条微博数据的特征）来解决这两个问题。<br>公开了地震数据集，来自维基百科和tweet.</p>
<p>提出了一个新颖的评估方法（Lenient evaluation），更适合于这种不十分精确的实验结果评估。</p>
<p>distant supervision方法用于自动构造训练数据,不用手工标注训练数据，值得注意这种方法的使用。</p>
<p>使用的是非结构化，信息繁杂的微博数据，而不是干净整洁的新闻报道数据。</p>
<p>当时这篇论文看的不仔细，CRF具体如何用于实验当中，还得细看。</p>
<ol>
<li>An Overview of Event Extraction from Text 2011<br>2011年以前的事件抽取综述</li>
</ol>
<p>主要介绍了3类方法：数据驱动的方法，基于专家知识的模板匹配方法，杂交方法</p>
<p>表格列出了论文参考文献中的方法，比较了各类方法的优缺点。</p>
<p>当时本文都是基于干净的在线新闻文本做事件抽取，很少在微博这种短小不精确的文本上做事件抽取。 </p>
<ol>
<li>Cross-document Event Extraction and Tracking-Task, Evaluation, Techniques and Challenges_RANLP2009</li>
</ol>
<p>跨文档的事件链条构建，以人名为中心词，跨文档事件聚合，融入背景知识的时间参数纠正，并按照时间排序各个事件。</p>
<p>介绍了ACE任务中的名词术语和工作目标。提出了跨文档事件抽取的任务，融入了背景知识（相关文档和维基百科数据）来纠正事件参数（时间），并且提出了新的评价方法（Browshing cost，tempotal correlation），最后构建了一个以单文档事件抽取为基础的跨文档事件抽取系统。</p>
<p>需要详细了解ACE任务：Automatic Content Extraction, 引用量最高的论文。</p>
<ol>
<li>SenticNet 3: A Common and Common-Sense Knowledge Base for Cognition-Driven Sentiment Analysis 2014 AAAI<br>是一个情感极性分析，观点挖掘的知识库。结合预处理，语义分析，观点目标词检测，能够组成一个完整的认知驱动的观点挖掘系统。<br>现在有三个版本，公开了数据资源，用xml格式存储，数据库支持。<br>energy flow能量子流动，情感分类模型：Hourglass of Emotions，RDF－XML数据库存储，SBoCs。<br>情感极性分类：<br> 1:keyword spotting<br> 2:lexical affinity<br> 3:statistical methods<br> 4:concept-level approaches</li>
</ol>
<p>公共知识库：<br>DBPedia,Freebase,YAGO,WiKipedia,NELL,Probase.</p>
<p>常识知识库：<br>Cyc,Open Mind Commom Sense(OMCS),ConceptNet,WordNet,WNA.</p>
<p>UK－national health service(NHS),PatientOpinion,英国公共医疗服务评价。</p>
<ol>
<li>Towards a Chinese Common and Common Sense Knowledge Base for Sentiment Analysis LNAI 2012<br>先分析了互联网上中文使用者和数据量将超越英文，然后说了构建中文情感分析知识库的必要性。</li>
</ol>
<p>混合Probase and ConceptNet来构建情感分析知识库，然后运用机器翻译方法（模糊匹配）将其翻译为中文。<br>短文</p>
<p>没有做evaluation。Conclusion写的很好。</p>
<ol>
<li>Isanette: A Common and Common Sense Knowledge Base for Opinion Mining 2011 ICDM_wordshop</li>
</ol>
<p>混合Probase and ConceptNet来构建情感分析(观点挖掘)知识库，并解决了两个问题：1，多个词表示同一个意思（这个很有启发，事理图谱也会遇到同样的问题）2，降低数据的稀疏性，并用降维技术检测了含有的否定形式。<br>最终知识库的形式：<br>    a matrix 340,000 × 200,000 whose rows are instances such as ‘birthday party’ and ‘china’, whose columns are concepts like ‘special occasion’ and ‘country’, and whose values indicate truth values of assertions.</p>
<p>然后利用该知识库做reasoning和opinion mining：<br>reasoning：1将知识库用向量空间进行表示（Vector Space Representation），2进行<br>语义聚类（Semantic Clustering）</p>
<p>opinion mining：1观点挖掘引擎（Opinion Mining Engine），2评价（Evaluation）</p>
<p>结论：混合Probase and ConceptNet构建了开放领域的知识库，可以有效进行reasoning，进而完成观点挖掘，<br>情感分析，文本自动分类等任务。</p>
<p>好像没有开放资源：Isanette。<br>利用该知识库做reasoning和opinion mining的部分有待细读。</p>
<p>7：A Survey on Truth Discovery SIGKDD,韩家炜<br>有待精读。</p>
<p>任务定义：<br>principle：<br>input：<br>output：<br>data：</p>
<p>8：中文社交媒体谣言统计语义分析_rumor2015，中国科学，刘知远，张乐，涂存超</p>
<p>微博谣言影响力，产生与消亡的特点，谣言分类，时序分析，自动辟谣框架（贝叶斯公式，专家发现）<br>具体专家发现方法见2012年的草考文献17</p>
<p>9：知识表示学习研究进展 刘知远，计算机研究与发展，2015</p>
<p>知识表示学习简介：基本概念（独热表示，大量相关研究），理论基础（模拟人类大脑，多个突触状态表示复杂知识），典型应用，主要优点（相似度计算效率高，有效缓解数据稀疏，异质信息融合）</p>
<p>知识表示学习主要方法：<br>距离模型：SE<br>单层神经网络模型：SLM<br>能量模型：SME<br>双线性模型：LFM，效果较好<br>张量神经网络模型：NTN<br>矩阵分解模型：与LFM思想类似，具有长处<br>翻译模型：TransE，主流模型<br>其他模型：全息表示模型</p>
<p>主要挑战与已有解决方案：<br>复杂关系建模：多个针对transE的改进方案<br>多源信息融合：<br>关系路径建模：多条路径推理</p>
<p>未来研究方向展望：<br>面向不同知识类型的知识表示学习<br>多源信息融合的知识表示学习<br>考虑复杂推理模式的知识表示学习<br>其他研究方向：面向大规模知识库的在线学习和快速学习，知识分布式表示的应用</p>
<p>启发idea：词向量为什么必须是同一维度的？复杂向量维度长，简单向量维度短。是否有一种度量方法可以有效度量不同维度文本之间的相似性-是否仍可以采用余弦相似度？</p>
<p>science-认知科学研究成果将知识类型分为四类：树状关系（生物分类系统），二维网格关系（地理位置），单维顺序关系（偏序关系），有向网络关系（关联或因果关系）<br>事理图谱是否是专门面向第四种知识类型的知识库？</p>
<ol>
<li>The Weltmodell: A Data-Driven Commonsense Knowledge Base，2014，短文,德国<br>大数据驱动的常识知识库</li>
</ol>
<p>知识获取：<br>所使用数据集，事实抽取方法，互信息计算，相似度计算（concept，statement）,知识库构建结果。</p>
<p>展示：web</p>
<p>前景与展望：<br>改进事实抽取模块，集成外部知识库，增加可视化展示选项，加入用户反馈。<br>需要改进的：事实消歧，知识库补全，常识推理-不仅仅实现尝试推理，而且让人可以理解推理过程</p>
<ol>
<li>A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books,2013,google</li>
</ol>
<p>介绍了谷歌的公布的一个Syntactic-Ngrams语料库，这个语料库体积非常大，适合用来做各种nlp任务。</p>
<ol>
<li>Verb Pattern: A Probabilistic Semantic Representation on Verbs，2016AAAI,复旦大学GBM lab，cuiwanyun</li>
</ol>
<p>本工作针对英语动词的一词多义现象进行研究，提出了一种动词一词多义现象的表示方法：抽象出动词搭配的概念和特定搭配两种形式作为动词一词多义的表示。<br>动词：概念<br>动词：特定搭配<br>训练语料用的就是上面谷歌公布的Syntactic-Ngrams语料以及Probase知识库。</p>
<ol>
<li>KRAKEN: N-ary Facts in Open Information Extraction,Weltmodell作者，NAACL，2012</li>
</ol>
<p>提出了一个开放信息抽取系统，可以抽取多元知识，知识的完整性更好，准确率更高，每句话知识抽取率更高。<br>基于传统的分词，词性标注，句法依存分析，然后提出一个规则集合来抽取知识。   对噪声数据（语法不合规则）抽取质量不高，因为依存分析的准确率会下降。</p>
<ol>
<li>Learning from the Web: Extracting General World Knowledge from Noisy Text，AAAI，2010<br>本工作对于构建事理图谱是一个很好的借鉴。</li>
</ol>
<p>USA罗切斯特大学,从大规模网页博客和维基百科数据中抽取常识知识，是一个系列工作，从2001年到2012年一直在做的一个工作。<br>开发了一个英文常识知识抽取系统：Knext。<br>本文定义了常识知识的形式：</p>
<p>Information extraction efforts, e. g. Banko et al. (2007), have focused on learning facts about specific entities, such as that Alan Turing died in 1954 or that the capital of Bahrain is Manama. Knowledge bases of such facts are quite useful, but getting to human-level AI seems to depend less on this specific knowledge than it does on the most basic world knowledge – our commonsense un- derstanding of the world.</p>
<p>for the Wikipedia sentence “The emperor was succeeded by his son, Akihito”, what we seek to (and do) learn is that ‘AN EMPEROR MAY BE SUCCEED -ED BY A SON’ and ‘A MALE MAY HAVE A SON’ – not the specific information about Emperor Sho ̄wa and his son. Thus, hav- ing been written as a repository of information, which most weblogs are not, is not a clear advantage for Wikipedia as a resource for extracting background knowledge. </p>
<p>本文回到了四个问题：<br>    随着输入语料的不断增加，抽取到的知识量也在不断增加<br>    尽管博客和维基百科数据质量不同，抽取到的知识质量却大致相同<br>    对抽取到的原始知识进行规则过滤，可以大幅度提高知识质量<br>    从博客和维基百科抽取到的常识知识互相的覆盖率很小，尽管随着数据规模增加覆盖率也在不断缓慢增加</p>
<p>本文还提供了一个可借鉴的知识库评价方法：<br>手工评价，随机抽取一定量的知识，将每一条知识根据一定的标准评价为5个分值，然后计算平均得分来衡量知识的质量。</p>
<p>一个有用的考虑：对于超大规模的数据来说，知识抽取更加注重准确率，而不是recall。</p>
<p>最后一点：论文并没有说明所构建的知识库在实际AI系统当中的用处，所以我们应该注重事理图谱在工业中的应用价值，而不仅仅是想当然构建出一个玩物知识库。</p>
<ol>
<li><p>Learning Textual Graph Patterns to Detect Causal Event Relations,AAAI,2010<br>事件因果关系检测，提出了一种新的特征模板，利用SVM作为分类器，打败了前一个代表性工作的实验结果。<br>数据集已经标注好了两个事件之间是否有因果关系，且整个数据集只有1000条，对事理图谱构建帮助不大。</p>
</li>
<li><p>Mining Commonsense Knowledge From Personal Stories in Internet Weblogs，AKBC,2010</p>
</li>
<li><p>Commonsense Causal Reasoning Using Millions of Personal Stories，AAAI,2011   </p>
</li>
<li><p>Open-domain Commonsense Reasoning Using Discourse Relations from a Corpus of Weblog Stories，NAACL，2010</p>
</li>
<li><p>Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning，AAAI,2011 </p>
</li>
<li><p>Automatic Knowledge Base Construction using Probabilistic Extraction, Deductive Reasoning, and Human Feedback, 2012,AKBC</p>
</li>
<li><p>Automatic Construction of Inference-Supporting Knowledge Bases,AKBC,2014,best paper</p>
</li>
<li><p>KELVIN: a tool for automated knowledge base construction,NAACL,2013</p>
</li>
</ol>
<p>下面几篇论文需要总结：</p>
<ol>
<li><p>Advances in Automated Knowledge Base Construction,2013,AKBC</p>
</li>
<li><p>Deep Learning for Event-Driven Stock Prediction，dingxiao,IJCAI,2015</p>
</li>
<li><p>Using Structured Events to Predict Stock Price Movement: An Empirical Investigation,dingxiao EMNLP,2014</p>
</li>
<li><p>Mining the Web to Predict Future Events,WSDM,2013,Kira Radinsky</p>
</li>
<li><p>Learning Causality for News Events Prediction,WWW,2012,Kira Radinsky<br>非常重要：<br>用因果模板构建因果关系对作为训练数据，构建因果事件图谱，实现事件泛化<br>用多种评价方法从多个角度对事件预测模型进行了评价。需要人工评价</p>
</li>
<li><p>因果关系及其在社会媒体上的应用研究综述,赵森栋，2014<br>因果分类：常识因果，浅层因果，深层因果</p>
</li>
</ol>
<p>重要问题：学习词或短语语义之间的逻辑关系(尤其是 因果关系)进而表示成知识库的形式,与不断地优化因果抽取算法相比更有意义.</p>
<p>本质因果推断的三类方法：<br>    1.随机对照实验<br>    2.准实验设计方法<br>    3.联合模型方法：图模型和虚拟事实模型</p>
<p>问题与挑战：<br>    因果对识别：模板匹配和规则（常识因果）<br>    因果模型的适用性-泛化能力<br>    客观的评价指标<br>    社会媒体上的因果分析-值得做</p>
<p>未来研究方向：社会媒体上的因果知识的识别和抽取，基于因果图模型的应用研究（贝叶斯网络）<br>    1.理论创新：<br>    2.社会媒体上的应用：因果分析，基于因果知识的预测和推理</p>
<p>29.Event causality extraction based on connectives analysis，赵森栋，2015，neurocomputing<br>这是森栋师兄的第二个工作，从单个句子中抽取因果关系。第一步是识别句子内的因果单元，第二步确定两个单元之间有无因果关系，以及因果关系的方向。<br>创新之处在于1.引入了一个新的因果关联类别特征特征，2.提出了一个新的模型Hidden Naive Bayes model,可以识别特征之间的互相作用关系，降低过拟合。</p>
<ol>
<li><p>事件常识的获取方法研究，王亚，曹存根，计算机科学，2015<br>事件分类，框架表示，事件常识知识获取，事件公理获取，与FrameNet的比较。<br>整个工作都是基于手工标注的，耗费大量人力物力。<br>需要一个自动事件挖掘系统来挖掘相应的事件常识。</p>
</li>
<li><p>微博知识图谱构建方法研究，杜亚军，2015，西华大学<br>本文是对知识图谱构建的一个很好的综述文章，相关研究描述的很清楚。尤其写明了知识图谱的评价方法。<br>[62],[63],[64],[65],[66],[67]<br>提出了微博知识图谱构建需要解决什么问题，但是并没有给出任何解决方案。但是本文提出的问题也是消费事理图谱构建的<br>面临的问题，所以十分具有借鉴意义。<br>概念对象：人物，事物，地点，事件，话题<br>概念、关系重复，一词多义，同义扩展，多义扩展，知识图谱的更新<br>分析：part3.4<br>应用：part3.5</p>
</li>
<li><p>基于层叠条件随机场的事件因果关系抽取*，PR &amp; AI,付 剑 锋<br>重要参考文献：[15]。<br>这篇文章的借鉴意义也很大，利用CRF建模事件之间的关系。可以作为消费事理图谱构建的一个可选方法。<br>需要标注语料，标注事件之间的关系。但是本文没有进行事件抽取，标注好事件以及因果关系，然后进行因果关系识别。<br>本文也定义了事件的概念。这里的事件只是一个事件触发词。</p>
</li>
<li><p>从大规模web语料中获取常识语料，朱耀，曹存根，2008<br>p2 </p>
</li>
<li><p>事件时序关系建模的研究与实现，张绍臣，2009 </p>
</li>
</ol>
<p>文景转换：参考文献</p>
<p>国内外很多项目已经对文景转换进行了研究,AT&amp;T 实验室研究开发的<br>WordsEye 系统[1],能够根据简单的文本描述,通过对含有几千个三维图形<br>的模型库查找,生成静态的三维图像;隆德大学开发的 CarSim  系统能够处<br>理交通领域的真实事故报告,生成交通事故报告描述的动态场景[2],具有很<br>高的实用价值;我国中科院陆汝钤院士带领开发的“天鹅”系统[3]实现中文<br>故事到动画片,全自动辅助动画的自动生成。</p>
<p>几年来哈尔滨工业大学机器智能与翻译实验室在文景转换方面也取得了一些成果,尤其在三维空间物体摆放以及基于文本到动画的生成都有所收获[4]。</p>
<p>常识的定义和ConceptNet<br>事件的定义和相关研究：事件=表示事件的动词+动词的受事者。open,开，防晒<br>事件时序关系的分类和相关研究：TimeML 和 ACE 分别对时序关系进行了不同的分类。 </p>
<p>本研究使用 TimeBank 1.2 语料 182 篇文章。在本文的后续研究中使用到<br>Evita 来识别事件和 Classifier,BLinker 来识别和提取事件的时序关系。在<br>后边的章节中会对这几个工具进行进一步的说明。 </p>
<p>ACE 对时序关系的定义,主要集中在事件与时间表达式的关系,很少<br>涉及事件与事件之间的时序关系,而且多数关系相互区别度不高,主要依靠<br>是否存在明显的标记来区分不同的时序关系。 </p>
<p>综合 TimeML 和 ACE 对时序关系的定义,将区别度很小的不同时序关<br>系 进 行 归 类 , 得 到 本 课 题 定 义 的 3 类 时 序 关 系 : Before , After ,Simultaneous,分别表示两个事件之间发生的“前”,“后”和同时发生的时序关系。 </p>
<p>事件的识别（动词名词）和抽取（受事者抽取）：</p>
<p>事件合并：（完全相同的事件和相似事件，事件相似度计算方法）</p>
<p>待解决的问题：<br>中文处理<br>受事者提取<br>高频噪音事件节点删除<br>评测方法</p>
<ol>
<li>事件时序关系识别的研究与实现，孙辉，2010</li>
</ol>
<p>本课题中中文研究现状以及分析部分特别有帮助，需要找到相关论文来看。</p>
<p>事件关系抽取是该项目的主要研究内容，而事件时序关系作为事件关系的<br>一种，旨在研究事件在时间上的先后顺序关系。本研究课题希望通过结合计算<br>语言学知识与统计机器学习方法，搭建事件时序关系机器自动识别平台。</p>
<p>本研究课题以英文为语言基础，以事件时序关系作为研究内容，将事件时<br>序关系知识库作为语义资源，以搭建事件时序关系识别模型作为研究目的。 </p>
<p>（1）应对事件时序关系识别模型的特征空间进一步细化。本研究只对每个<br>特征对模型的影响给出评价，并未考虑特征组合对模型的影响。<br>（2）用统计机器学习方法搭建基于中文的事件时序关系识别模型。<br>（3）本研究用 VerbNet 词汇资源搭建句法框架构造规则进行事件受事者识<br>别，效果不是很理想，可以考虑用基于统计机器学习的方法构造识别模型。</p>
<p>Evita(Events in text Analyzer)是 ARDA 资助的 TARSQI 研究框架下的事件识<br>别工具<br>[27]<br>。Evita 作为 TTK(Tarsqi Toolkit)工具包中的一个组件，它主要用来<br>进行事件识别和事件关联语法特征分且没有应用领域的限制，只需在事件标注<br>前用 AlembicWorkbench 标注器和 Chunker 工具对文本进行词性标注和 Chunk<br>预处理。Evita 的识别性能达到 80.12%的 F-measure。</p>
<p>如果不是动词性事件，直接断定事件受事者为空。 </p>
<p>从表 4-2 中可以看出，该模块的平均准确率为 57.00%。性能低下的原因在<br>于 VerbNet 句法框架的匹配率很低，Wiki 中编写者多用复杂句，而 VerbNet 中<br>的句法框架则是根据简单句建立起来的，这用情况下的事件受事者很难提取。</p>
<p>学者们一般将基于本体的语义相似度计算方法划分为 4 类[31]：基于距离的<br>语义相似度计算（Edge  Counting  Measures）、基于内容的语义相似度计算<br>（Information Content Measures）、基于属性点的语义相似度计算（Feature-based<br>Measures）和混合式语义相似度计算（Hybrid Measures）。</p>
<ol>
<li><p>基于语义依存线索的事件关系识别方法研究，马彬 洪宇† 杨雪蓉 姚建民 朱巧明，2013<br>话题检测与跟踪，事件关系识别，具有参考价值<br>还没看完</p>
</li>
<li><p>BUEES: a Bottom-Up Event Extraction System,Xiao Ding<br>包含两个工作：<br>First：事件类型自动抽取。动词细分类。   Trigger抽取，Clustering，Filter。<br>Second：事件arguments抽取。</p>
</li>
<li><p>Building Chinese Event Type Paradigm Based on Trigger Clustering，Xiao Ding<br>本文工作包含在上一篇论文中，是BUEES的一个子工作。</p>
</li>
<li><p>ZORE: A Syntax-based System for Chinese Open Relation Extraction</p>
</li>
</ol>
<p>关系抽取，弱动词结构的提取。<br>关系抽取的候选：三种类型。<br>语法语义模板匹配。<br>LR置信度结果过滤。<br>跟事件抽取本质是不一样的，事件抽取要有事件触发词。关系抽取关注两个NP之间的关系，最少要有两个NP结构。或者更多的NP结构则称为<br>N-ary facts。</p>
<ol>
<li>Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!,IBM,EMNLP,2013<br>在学术界和工业界使用rule_based方法差异很大，主要在于两个社区的的评价指标存在巨大差异：方法是否耗时费力的定义有所不同，所耗费的软件硬件资源学术界一般不报告。作者很担心在大数据信息抽取时代这种gap会越来越扩大，最终对两个社区都没有好处，最后提出了3个设想来缓解这种gap。</li>
</ol>
<p>最后作者提出的方法还没认真看。</p>
<ol>
<li>A Domain-independent Rule-based Framework for Event Extraction，University of Arizona, Tucson, AZ, USA，<br>IJCNLP，2015</li>
</ol>
<p>构建了一个领域独立的基于规则的开源事件抽取框架，有web界面，可视化匹配结果。<br>可以基于api编写抽取规则。</p>
<ol>
<li><p>Storybase: Towards Building a Knowledge Base for News Events,The Pennsylvania State University<br>University Park, PA 16802, USA, ACL IJCNLP 2015<br>写作的参考论文。<br>这篇论文构建新闻事件的知识库，提供了检索界面。<br>本文结构可以作为很好的参考，还有图形图标的绘制。<br>参考价值较大。<br>related work列出的几篇参考文献值得看看。</p>
</li>
<li><p>MPQA 3.0: An Entity/Event-Level Sentiment Corpus,Lingjia Deng,Intelligent Systems Program , University of Pittsburgh,NAACL,2015 </p>
</li>
</ol>
<p>MPQA是一个知识库，从实体，事件层次上来构建一个情感分析知识库。文本介绍了在MPQA2.0的基础上添加此信息的标注体系。<br>并分析了该知识库对于情感分析的作用。</p>
<ol>
<li><p>Research on Event Prediction Algorithm Based on Event Sequence Semantic, 西北大学，2009,FSKD<br>该研究质量比较低</p>
</li>
<li><p>Liu Ting et al. - 2007 - Subdividing verbs to improve syntactic parsing,电子学报</p>
</li>
</ol>
<p>最大熵模型实现动词细分类：8个类别。<br>然后细分类结果用于句法分析任务，发现动词细分类可以提高句法分析的性能。<br>2007年马金山，早期工作，需要找实验室车老师，或者哪位师兄问一问相关工作的实现。</p>
<ol>
<li><p>The Annotation of Event Schema in Chinese，北语，杨尔宏老师。<br>中文事件标注体系。<br>参考价值不大。</p>
</li>
<li><p>Using Textual Patterns to Learn Expected Event Frequencies, Jonathan Gordon,Department of Computer Science University of Rochester Rochester,2012, AKBC<br>从文本中匹配获取常识事件发生的频率。</p>
</li>
</ol>
<ol>
<li>Web mining for event-based commonsense knowledge using lexico-syntactic pattern matching and semantic role labeling, Expert Systems with Applications journal,2010, taiwan</li>
</ol>
<p>构造query，输入搜索引擎，处理返回的文本。<br>常识事件的定义和我们不同。<br>更像自动构造的concept net。<br>典型事件比如：Dog Eat Foods。</p>
<ol>
<li>Ranking Multidocument Event Descriptions for Building Thematic Timelines, coling 2014,法国</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;深度学习相关论文&quot;&gt;&lt;a href=&quot;#深度学习相关论文&quot; class=&quot;headerlink&quot; title=&quot;深度学习相关论文&quot;&gt;&lt;/a&gt;深度学习相关论文&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;机器学习系统，隐藏多少技术债？Hidden Technical Debt in 
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="http://yoursite.com/2015/04/15/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2015/04/15/集成学习/</id>
    <published>2015-04-15T07:47:55.000Z</published>
    <updated>2016-11-06T04:34:49.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="看《a-course-in-ML》第十一章ensemble-learning所做的笔记"><a href="#看《a-course-in-ML》第十一章ensemble-learning所做的笔记" class="headerlink" title="看《a course in ML》第十一章ensemble learning所做的笔记:"></a>看《a course in ML》第十一章ensemble learning所做的笔记:</h3><p>In this chapter, you will learn about various ways of combining into ensembles. One of the shocking results we will see is that you can take a learning model that only ever does slightly better than chance, and turn it into an arbitrarily good learning model, through a technique known as boosting. You will also learn how ensembles can decrease the variance of predictors as well as perform regularization.</p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives:"></a>Learning Objectives:</h3><p>Implement bagging and explain how it reduces variance in a predictor.</p>
<p>实现bagging并解释为什么它可以降低方差<br><a id="more"></a></p>
<p>Explain the difference between a weak learner and a strong learner.</p>
<p>解释弱学习器和强学习器的不同</p>
<p>Derive the AdaBoost algorithm.</p>
<p>提出AdaBoost-adaptive boosting algorithm</p>
<p>Understand the relationship between boosting decision stumps and linear classification.</p>
<p>理解决策树桩的提升和线性分类器的关系</p>
<p>boosting方法的优点：</p>
<p>比随机猜测好一点点的弱分类器提升为一个很好的分类器</p>
<p>天生并行，训练和测试效率更高</p>
<p>降低方差，实施正则化的另一种方法</p>
<h3 id="11-1-多个分类器投票"><a href="#11-1-多个分类器投票" class="headerlink" title="11.1 多个分类器投票"></a>11.1 多个分类器投票</h3><p>训练很多基础分类器：DT,perceptron,KNN,NN of different architectures,多者胜出</p>
<p>不是所有分类器都会犯同一个错误，如果每一个错误只有少数分类器犯下，那么你就会得到优化的分类器</p>
<p>不幸的是不同的算法倾向于饭同一类错误</p>
<p>对降低方差有效</p>
<p>二分类容易扩展到多分类问题，但不容易扩展到一下问题，因为很难得到相同的输出：回归：均值；排序，聚类需要不同的方法</p>
<p>在一个相通数据集上训练或者平分原来的数据集都不是很好的方法</p>
<p>一个数据集获得多个数据集：bootstrap resampling，原理：独立同分布，bagging</p>
<p>一个样例不被选中的概率：1-1/N   在一个集合中不出现的概率1-1/N的N次方，1/e==0.3679    N=1000（集合样本数量）是已经精确到4位小数</p>
<p>所以约有63%的原始样本会出现在所有的集合中 </p>
<p>正则化-降低方差：通过使用超参数或bagging方法 </p>
<p>尽管每一个分类器都overfit，但是对不同的特征overfit，通过投票，可以去除掉很大一部分overfit</p>
<p>图形对比：错误率-DT depth   错误率-num of bags</p>
<h3 id="11-2-boosting-weak-learners"><a href="#11-2-boosting-weak-learners" class="headerlink" title="11.2 boosting weak learners"></a>11.2 boosting weak learners</h3><p>提升弱学习器：想起文件压缩-不断压缩直至1b</p>
<p>强学习算法定义-第十章：PAC (probably approximately correct)learning，以成功率y学习到一个算法,错误率必须降低到e</p>
<p>可以容易得到一个弱学习器：49%错误率，只要比随机猜测好就行</p>
<p>boosting相对来说是一种学习框架而不是一个算法</p>
<p>特定的boosting algorithm-AdaBoosting-adaptive boosting algorithm</p>
<p>第一个实用的boosting算法：多项式复杂度，不需定义很多的超参数，自动适应训练数据</p>
<p>与准备考试相比较，答对的问题降低注意力，答错的问题提高注意力（权重）</p>
<p>在训练数据上的权重分布d（初始化每个样本权重都是1/N），在这个有权重的数据集上训练弱学习器（假设可以接受加权样本数据，基本都是可以的），Fk的错误率用来决定这个函数的适应参数a，表示这个分类器结果的重要性。只要分类器的错误率小于50%，那么a就会大于零，如果错误率为0，那么这个分类器的适应参数就会无限大，相当于只需要这一个分类器的结果就够了。</p>
<p>a计算出来后，权重分布d在新一轮迭代中得到更新。分类正确的样本权重下降，分类错误的样本权重上升，Z是正则项，保证d的和为1。</p>
<p>算法最终返回结果：多个分类器的加权投票结果</p>
<p>一个具体的例子：对每个样本加权之后，如果要取得小于50%的错误率，就要有所更新。</p>
<p>shallow DT：with small depth</p>
<p>图形对比：一层的DT,多个boost  和  一个DT,深度逐渐增加的对比</p>
<p>decision stumps-AdaBoost起到了线性分类器的效果</p>
<p>一个具体的例子：<br>更进一步：最近的研究表明,AdaBoost算法优化目标为exponential loss</p>
<p>另一个例子：linear classifier-AdaBoost起到了两层神经网络的效果<br>K个线性分类器，K个隐藏单元，<br>仅仅是结构相同-但是参数学习方法不同，一旦学习到了线性分类器的W和b参数，就再也不会更新了。</p>
<h3 id="11-3-random-ensembles随机集成算法"><a href="#11-3-random-ensembles随机集成算法" class="headerlink" title="11.3 random ensembles随机集成算法"></a>11.3 random ensembles随机集成算法</h3><p>选择决策树的结构非常耗时，树桩算法很高效</p>
<p>固定树的结构，随机选择特征（一个特征甚至可以在一个分支当中多次出现）：随机森林<br>D（数据），depth（树深），K（决策树的数量）</p>
<p>最终分类器：多个随机树的投票</p>
<p>这种方法效果很好：不同特征之间不相关时效果最好，因为每一棵树可能用到的特征很少。一些树会query无用的特征，产生随机猜测结果，可以作为噪声数据，最终只有好的决策树有效。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;看《a-course-in-ML》第十一章ensemble-learning所做的笔记&quot;&gt;&lt;a href=&quot;#看《a-course-in-ML》第十一章ensemble-learning所做的笔记&quot; class=&quot;headerlink&quot; title=&quot;看《a course in ML》第十一章ensemble learning所做的笔记:&quot;&gt;&lt;/a&gt;看《a course in ML》第十一章ensemble learning所做的笔记:&lt;/h3&gt;&lt;p&gt;In this chapter, you will learn about various ways of combining into ensembles. One of the shocking results we will see is that you can take a learning model that only ever does slightly better than chance, and turn it into an arbitrarily good learning model, through a technique known as boosting. You will also learn how ensembles can decrease the variance of predictors as well as perform regularization.&lt;/p&gt;
&lt;h3 id=&quot;Learning-Objectives&quot;&gt;&lt;a href=&quot;#Learning-Objectives&quot; class=&quot;headerlink&quot; title=&quot;Learning Objectives:&quot;&gt;&lt;/a&gt;Learning Objectives:&lt;/h3&gt;&lt;p&gt;Implement bagging and explain how it reduces variance in a predictor.&lt;/p&gt;
&lt;p&gt;实现bagging并解释为什么它可以降低方差&lt;br&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>shell基本编程自己用过的一些经典例子</title>
    <link href="http://yoursite.com/2015/04/15/shell%E8%84%9A%E6%9C%AC%E6%A0%87%E7%A8%8B%E8%87%AA%E5%B7%B1%E7%94%A8%E8%BF%87%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%E4%BE%8B%E5%AD%90/"/>
    <id>http://yoursite.com/2015/04/15/shell脚本标程自己用过的一些经典例子/</id>
    <published>2015-04-15T05:37:48.000Z</published>
    <updated>2016-11-06T04:34:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文本行中各列打乱：<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk <span class="symbol">'BEGIN</span>&#123;srand()&#125;&#123;<span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=NF;i++) b[rand()NF]=$i&#125;<span class="keyword">END</span>&#123;<span class="keyword">for</span>(x <span class="keyword">in</span> b)printf <span class="string">"%s "</span>,b[x]&#125;' data</div></pre></td></tr></table></figure></p>
<p>文本行中各行打乱：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk '<span class="keyword">BEGIN</span>&#123;srand()&#125;&#123;b[<span class="keyword">rand</span>()NR]=$<span class="number">0</span>&#125;<span class="keyword">END</span>&#123;<span class="keyword">for</span>(x <span class="keyword">in</span> b)print b[x]&#125;<span class="string">' data</span></div></pre></td></tr></table></figure></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">shuf</span> -n1000 <span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure>
<a id="more"></a>
<p>以a.txt作为pattern查找b.txt，实际是求2者交集：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -F -x -f <span class="selector-tag">a</span><span class="selector-class">.txt</span> <span class="selector-tag">b</span>.txt</div></pre></td></tr></table></figure></p>
<p>以a.txt作为pattern查找b.txt，显示不在a.txt中的数据，实际是求差集b-a：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -F -v -x -f <span class="selector-tag">a</span><span class="selector-class">.txt</span> <span class="selector-tag">b</span>.txt</div></pre></td></tr></table></figure></p>
<p>编码转换：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iconv <span class="_">-f</span> gb18030 -t utf8 filename</div></pre></td></tr></table></figure></p>
<p>以_为分隔符，第二列为键值排序，稳定排序（默认不稳定）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sort -t_ -k1,2  <span class="_">-s</span>  filename</div></pre></td></tr></table></figure></p>
<p>对大文件进行外部排序并去重，以temp目录作为缓存：<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">sort -u -T temp filename</span></div></pre></td></tr></table></figure></p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">sort</span> filename | uniq   -<span class="built_in">c</span>计数唯一的排序的记录 -d仅仅显示重复的记录 -u仅仅显示没有重复的记录</div></pre></td></tr></table></figure>
<p>在preview中打开awk的man文档：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">man</span> -t awk | <span class="keyword">open</span> -a Preview -<span class="built_in">f</span></div></pre></td></tr></table></figure></p>
<p>awk和cut简单用法：<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">awk</span> -F: -v <span class="string">'OFS=\t'</span> <span class="string">'&#123;print "all users are:",<span class="variable">$1</span>,<span class="variable">$2</span>,<span class="variable">$3</span>,<span class="variable">$4</span>,<span class="variable">$5</span>,<span class="variable">$6</span>,<span class="variable">$7</span>&#125;'</span> /etc/passwd | cut -f <span class="number">1</span>,<span class="number">6</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wc <span class="_">-l</span> 行数 -c字节数 -w字数</div></pre></td></tr></table></figure>
<p>查看端口:<br><figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">lsof</span> -i tcp:<span class="keyword">port</span> 将<span class="keyword">port</span>换成被占用的端口(如：8086、9998)</div></pre></td></tr></table></figure></p>
<p>查看自己的进程详细情况:<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">ps</span> -ef |<span class="keyword">grep</span> zyli</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本行中各列打乱：&lt;br&gt;&lt;figure class=&quot;highlight ada&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;awk &lt;span class=&quot;symbol&quot;&gt;&#39;BEGIN&lt;/span&gt;&amp;#123;srand()&amp;#125;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(i=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;;i&amp;lt;=NF;i++) b[rand()NF]=$i&amp;#125;&lt;span class=&quot;keyword&quot;&gt;END&lt;/span&gt;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(x &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; b)printf &lt;span class=&quot;string&quot;&gt;&quot;%s &quot;&lt;/span&gt;,b[x]&amp;#125;&#39; data&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;文本行中各行打乱：&lt;br&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;awk &#39;&lt;span class=&quot;keyword&quot;&gt;BEGIN&lt;/span&gt;&amp;#123;srand()&amp;#125;&amp;#123;b[&lt;span class=&quot;keyword&quot;&gt;rand&lt;/span&gt;()NR]=$&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;&lt;span class=&quot;keyword&quot;&gt;END&lt;/span&gt;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(x &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; b)print b[x]&amp;#125;&lt;span class=&quot;string&quot;&gt;&#39; data&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight haskell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;title&quot;&gt;shuf&lt;/span&gt; -n1000 &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;data&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="shell" scheme="http://yoursite.com/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>我的个人简历</title>
    <link href="http://yoursite.com/2015/04/15/%E6%88%91%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86/"/>
    <id>http://yoursite.com/2015/04/15/我的个人简历/</id>
    <published>2015-04-15T04:41:59.000Z</published>
    <updated>2016-11-06T04:34:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>####教育经历</p>
<ul>
<li>2008.08-2011.06 河南省林州市林州一中-高中</li>
<li>2011.08-至今 哈尔滨工业大学-本科</li>
</ul>
<p>####研究方向</p>
<ul>
<li>社会媒体处理</li>
<li>自然语言处理</li>
<li>机器学习</li>
<li>数据挖掘</li>
</ul>
<a id="more"></a>
<p>####我的个人信息</p>
<ul>
<li>哈尔滨工业大学本科在读，社会计算与信息检索实验室-SCIR</li>
<li>家乡:河南省林州市</li>
<li>手机:15629181795</li>
<li>QQ:1482612462</li>
<li>常用邮箱:elizhongyang@163.com</li>
<li>工作邮箱:zyli@ir.hit.edu.cn</li>
<li>新浪微博:<a href="http://weibo.com/u/2494805143" target="_blank" rel="external">我的新浪微博主页</a></li>
<li>个人主页:<a href="http://ir.hit.edu.cn/~zyli" target="_blank" rel="external">我的实验室个人主页</a></li>
<li>CSDN主页:<a href="http://blog.csdn.net/e6894853" target="_blank" rel="external">我的CSDN</a></li>
<li>博客园主页:<a href="http://www.cnblogs.com/zzllzy" target="_blank" rel="external">zzllzy的园子</a></li>
<li>github主页:<a href="https://github.com/eecrazy" target="_blank" rel="external">我的github</a></li>
<li>豆瓣:<a href="http://www.douban.com/people/54858206" target="_blank" rel="external">我的豆瓣个人主页</a></li>
</ul>
<p>####技能</p>
<ul>
<li>c/c++</li>
<li>python</li>
<li>shell</li>
<li>java</li>
<li>statistical machine learning</li>
<li>ruby on rails,css</li>
<li>mysql</li>
<li>hadoop</li>
<li>vim,emacs,sublime</li>
</ul>
<p>####兴趣</p>
<ul>
<li>mongodb</li>
<li>nodejs,javascripte,bootstrap</li>
<li>lisp</li>
<li>ios,swift</li>
</ul>
<p>####爱好</p>
<ul>
<li>游泳</li>
<li>跑步</li>
<li>LOL</li>
<li>街舞:breaking</li>
</ul>
<p>####工作经历</p>
<ul>
<li>2014.03-2014.04 学工办助理</li>
<li>2014.06.25-2014.09.30 百度NLP部门DQA小组实习生</li>
<li>2012.02-2012.06 实验学院C语言助教</li>
<li>2013.02-2013.06 软件学院C语言助教</li>
</ul>
<p>####项目经历</p>
<ul>
<li>百度知道问答对垃圾数据过滤系统</li>
<li>负责维护实验室电影票房预测系统</li>
<li>类Twitter微博系统</li>
</ul>
<p>####曾任职务</p>
<ul>
<li>班级团支书</li>
<li>哈工大微软俱乐部技术部副部长</li>
</ul>
<p>####所获奖励</p>
<ul>
<li>2013国家励志奖学金</li>
<li>校园机器人足球亚军</li>
<li>2013黑龙江省ACM省赛一等奖</li>
<li>2013ACM亚洲区预赛浙江邀请赛三等奖</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;####教育经历&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2008.08-2011.06 河南省林州市林州一中-高中&lt;/li&gt;
&lt;li&gt;2011.08-至今 哈尔滨工业大学-本科&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;####研究方向&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;社会媒体处理&lt;/li&gt;
&lt;li&gt;自然语言处理&lt;/li&gt;
&lt;li&gt;机器学习&lt;/li&gt;
&lt;li&gt;数据挖掘&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
