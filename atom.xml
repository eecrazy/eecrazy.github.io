<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>李忠阳的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-11-29T07:44:49.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhongyang Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>组织</title>
    <link href="http://yoursite.com/2016/11/29/%E7%BB%84%E7%BB%87/"/>
    <id>http://yoursite.com/2016/11/29/组织/</id>
    <published>2016-11-29T07:43:24.000Z</published>
    <updated>2016-11-29T07:44:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="国外大学"><a href="#国外大学" class="headerlink" title="国外大学"></a>国外大学</h1><ul>
<li><a href="https://cogcomp.cs.illinois.edu/" target="_blank" rel="external">Cognitive Computation Group</a></li>
</ul>
<a id="more"></a>
<h1 id="国外公司"><a href="#国外公司" class="headerlink" title="国外公司"></a>国外公司</h1><h1 id="国内大学"><a href="#国内大学" class="headerlink" title="国内大学"></a>国内大学</h1><h1 id="国内公司"><a href="#国内公司" class="headerlink" title="国内公司"></a>国内公司</h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;国外大学&quot;&gt;&lt;a href=&quot;#国外大学&quot; class=&quot;headerlink&quot; title=&quot;国外大学&quot;&gt;&lt;/a&gt;国外大学&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cogcomp.cs.illinois.edu/&quot;&gt;Cognitive Computation Group&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据集</title>
    <link href="http://yoursite.com/2016/11/24/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://yoursite.com/2016/11/24/数据集/</id>
    <published>2016-11-24T05:47:58.000Z</published>
    <updated>2016-12-13T02:50:38.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><a href="http://ir.hit.edu.cn/hit-cdtb/" target="_blank" rel="external">hit-cdtb</a></li>
<li><a href="">大词林</a></li>
<li><a href=""></a></li>
<li><a href="">LDC</a></li>
<li><a href="">北大</a></li>
<li><a href="https://www.seas.upenn.edu/~pdtb/" target="_blank" rel="external">pdtb</a></li>
<li><a href="">数据堂</a></li>
<li><a href="">微博名人堂</a></li>
<li><a href="">谣言数据集</a></li>
<li><a href="http://kevinchai.net/datasets" target="_blank" rel="external">kevin chai</a></li>
<li><a href="http://snap.stanford.edu/data/" target="_blank" rel="external">SNAP-Stanford Large Network Dataset Collection非常好的网络数据大集合</a></li>
<li><a href="http://socialcomputing.asu.edu/pages/datasets" target="_blank" rel="external">socialcomputing datasets</a></li>
<li><a href="https://sites.google.com/site/yangdingqi/home/foursquare-dataset" target="_blank" rel="external">foursquare-dataset</a></li>
</ul>
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://ir.hit.edu.cn/hit-cdtb/&quot;&gt;hit-cdtb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;大词林&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;LDC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;北大&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.seas.upenn.edu/~pdtb/&quot;&gt;pdtb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;数据堂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;微博名人堂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;谣言数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://kevinchai.net/datasets&quot;&gt;kevin chai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://snap.stanford.edu/data/&quot;&gt;SNAP-Stanford Large Network Dataset Collection非常好的网络数据大集合&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://socialcomputing.asu.edu/pages/datasets&quot;&gt;socialcomputing datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sites.google.com/site/yangdingqi/home/foursquare-dataset&quot;&gt;foursquare-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>event_graph_visualization</title>
    <link href="http://yoursite.com/2016/11/22/event-graph-visualization/"/>
    <id>http://yoursite.com/2016/11/22/event-graph-visualization/</id>
    <published>2016-11-22T09:53:24.000Z</published>
    <updated>2016-11-22T10:00:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="事件网络可视化"><a href="#事件网络可视化" class="headerlink" title="事件网络可视化"></a>事件网络可视化</h1><ul>
<li>第一版本<a href="img/event_graph1.svg">SVG格式</a>，<a href="img/event_graph1.png">PNG格式</a></li>
</ul>
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;事件网络可视化&quot;&gt;&lt;a href=&quot;#事件网络可视化&quot; class=&quot;headerlink&quot; title=&quot;事件网络可视化&quot;&gt;&lt;/a&gt;事件网络可视化&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;第一版本&lt;a href=&quot;img/event_graph1.svg&quot;&gt;SVG格式&lt;/a&gt;，&lt;a href=&quot;img/event_graph1.png&quot;&gt;PNG格式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CCIR2016参会记录</title>
    <link href="http://yoursite.com/2016/11/14/CCIR2016%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2016/11/14/CCIR2016参会记录/</id>
    <published>2016-11-14T11:33:51.000Z</published>
    <updated>2016-11-16T07:15:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="会议日期"><a href="#会议日期" class="headerlink" title="会议日期"></a>会议日期</h2><p>11月11日到11月13日</p>
<h2 id="地点"><a href="#地点" class="headerlink" title="地点"></a>地点</h2><p>广州华南理工大学，大学城中心酒店<br><a id="more"></a></p>
<h2 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h2><p>11号上午报到，下午青年论坛和tutorial</p>
<p>12号上午开幕式和两个特邀报告，以及Panel-Is IR shrinking or not？，下午sessionA和sessionB</p>
<p>13号上午两个特邀报告，下午session，评测和闭幕式</p>
<h2 id="此次参会想到的idea"><a href="#此次参会想到的idea" class="headerlink" title="此次参会想到的idea"></a>此次参会想到的idea</h2><p>用然后因为所以等等关联词构造大规模训练数据  并构造反例  这样的话可以用深度学习  相似度匹配（利用徐君tutorial中提到的方法）<br>利用大规模新闻语料构建模型，领域迁移，到旅行语料上，在旅行语料的小规模样本上进行优化训练，得到适用于旅行语料的模型，判断pair语义相关性。</p>
<p>Boosting   LSTM  CNN  相融合   可以用简单任务比如句子分类验证boosting在deeplearning上的有效性</p>
<p>基于事理图谱的应用问题，或者conceptnet </p>
<p>Query意图分析，意图识别</p>
<p>One shot learning &amp; Unsupervised learning </p>
<h2 id="游玩"><a href="#游玩" class="headerlink" title="游玩"></a>游玩</h2><p>小蛮腰-广州塔</p>
<p>夜游珠江</p>
<p>上下九-广州小吃</p>
<p>大学城骑行</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;会议日期&quot;&gt;&lt;a href=&quot;#会议日期&quot; class=&quot;headerlink&quot; title=&quot;会议日期&quot;&gt;&lt;/a&gt;会议日期&lt;/h2&gt;&lt;p&gt;11月11日到11月13日&lt;/p&gt;
&lt;h2 id=&quot;地点&quot;&gt;&lt;a href=&quot;#地点&quot; class=&quot;headerlink&quot; title=&quot;地点&quot;&gt;&lt;/a&gt;地点&lt;/h2&gt;&lt;p&gt;广州华南理工大学，大学城中心酒店&lt;br&gt;
    
    </summary>
    
      <category term="参会记录" scheme="http://yoursite.com/categories/%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>信息理论与编码</title>
    <link href="http://yoursite.com/2016/11/07/%E4%BF%A1%E6%81%AF%E7%90%86%E8%AE%BA%E4%B8%8E%E7%BC%96%E7%A0%81/"/>
    <id>http://yoursite.com/2016/11/07/信息理论与编码/</id>
    <published>2016-11-07T05:47:14.000Z</published>
    <updated>2016-11-21T03:48:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="考试备战"><a href="#考试备战" class="headerlink" title="考试备战"></a>考试备战</h1><h2 id="知识点整理"><a href="#知识点整理" class="headerlink" title="知识点整理"></a>知识点整理</h2><pre><code>一、信源熵与信道容量（计算，难度：中）
二、山农公式（计算，难度：易）？？？
三、马尔科夫（计算，难度：易）
四、霍夫曼（计算，难度：易）
五、译码准则（证明，难度：中）
六、线性分组码（1）（计算，难度：易）
七、线性分组码（2）（证明，难度：中-难）
八、循环码（计算，难度：中）
九、BCH码（计算，难度：难）
十、卷积码(电路)（综合，难度：中）
</code></pre><a id="more"></a>
<pre><code>循环码编码、译码电路（梅吉特、捕错、大数逻辑）无考点，R-S码无考点，卷积码电路为考点，卷积码译码（维特比、序列）无考点。
卷积码原理电路图、编码方法、状态图需要掌握，译码技术不作为本次考点。
</code></pre><h2 id="题型分类"><a href="#题型分类" class="headerlink" title="题型分类"></a>题型分类</h2><pre><code>试卷形式：

    一、选择，2分*8题；
    二、填空，2分*7题；
    三、四，简单计算，各10分；
    五、证明，10分；
    六、电路设计，10分；
    七、八，计算，各15分。

请大家注意，在题签、答题册、草稿纸（可能是2张）都要写序号与姓名，并且都要交回，交回材料不足或信息不全会影响大家成绩。

二、考试题型：
        （1）填空题，7道题*2分/题=14分；
        （2）选择题，8道题*2分/题=16分；
        （3）计算（证明）题，7道题*10分=70分；
    还是那句话，题型转变是件利弊参半的事情，15道小题，涉及的知识点的确增多了，但是很多题都是“送分”型的。我也不清楚小题和大题的时间分布，很有可能明天最终定稿的时候，把大题变为6道，10*4+15*2=70分，这种结构。
    填空和选择题涉及的都是基础概念与公式。计算证明题涵盖信道与信源、译码准则、循环码、BCH码、卷积码。没有任何超出讲授范围的试题。
</code></pre><h2 id="提醒"><a href="#提醒" class="headerlink" title="提醒"></a>提醒</h2><pre><code>记得带 **计算器** 和 **学生卡**
考试时间：下下周(12周)三 11月23日 下午4点到6点
</code></pre><h1 id="石硕老师几封历史邮件"><a href="#石硕老师几封历史邮件" class="headerlink" title="石硕老师几封历史邮件"></a>石硕老师几封历史邮件</h1><h2 id="主题：2013-满纸荒唐言，挑拣有用的看看吧。"><a href="#主题：2013-满纸荒唐言，挑拣有用的看看吧。" class="headerlink" title="主题：2013 满纸荒唐言，挑拣有用的看看吧。"></a>主题：2013 满纸荒唐言，挑拣有用的看看吧。</h2><pre><code>各位同学好:
    （1）考试时间地点：2013.11.22（周五），13:30---15:30，地点A12。
    （2）余素慵懒，终于今日早起，把题出完了，特奉上出题心得，供大家复习参考——
    如往年，十道大题，每题十分，试卷结构如下： 

     一、信源熵与信道容量（计算，难度：中）
     二、山农公式（计算，难度：易）
     三、马尔科夫（计算，难度：易）
     四、霍夫曼（计算，难度：易）
     五、译码准则（证明，难度：中）
     六、线性分组码（1）（计算，难度：易）
     七、线性分组码（2）（证明，难度：中-难）
     八、循环码（计算，难度：中）
     九、BCH码（计算，难度：难）
     十、卷积码（综合，难度：中）

    （3）循环码编、译码电路（梅吉特、捕错、大数逻辑）无考点，R-S码无考点，卷积码电路为考点，卷积码译码（维特比、序列）无考点。

    （4）关于补课：上周实在没能找到公共时间，加之截止今日，面告或邮件通知我需要补课的同学共3位，请此3位同学约时间前来，如3位实在没有共同时间，分别找我亦无不可。若甚，请3位自修最后一堂内容：卷积码相关知识以及实验安排，其中卷积码原理电路图、编码方法、状态图需要掌握，译码技术不作为本次考点。
        特别说明：补课仅为最后一堂课因时间冲突未能参加之同学而安排，无其他内容。信息与编码理论不设答疑已有纪年，请大家不必浪费时间周折，自误误人。

    （5）实验提交时间为12月9日（周一）、10日（周二），为减少工作量，烦请班长或各系负责同学帮助收起，实验程序包、实验报告纸质版，请在上述时间交至本课程助教许恩玮博士生处（电话15645630008，实验室2A-1109），请尽量不要提前或延迟提交，谢谢。

    （6）考试需用计算器，请自备，原则上不许在考场上借用，严禁用手机或pad等代替！（喜欢脑算对数的天才可忽略此条）。同时请大家注意考试纪律，莫忘携带学生证（卡）。

    （7）关于对教材、讲义得勘误补缺，希望得到大家的指正、反馈，在适当范围内，会按顾老师的建议，将此部分作为实验成绩的参考。

    此致，
敬礼！
身体健康！
万事如意！
哈尔滨工业大学
电子与信息工程学院
通信技术研究所
石硕 敬上
TEL：86-451-86413513-8224 
M.P.：159-4519-1498
</code></pre><h2 id="主题：2015-考试信息"><a href="#主题：2015-考试信息" class="headerlink" title="主题：2015 考试信息"></a>主题：2015 考试信息</h2><pre><code>各位同学好！

    一、信息论考试时间、地点：第12周周一（2015.11.30），13:30-15:30，A22教室。
    二、考试题型：
        （1）填空题，7道题*2分/题=14分；
        （2）选择题，8道题*2分/题=16分；
        （3）计算（证明）题，7道题*10分=70分；
    还是那句话，题型转变是件利弊参半的事情，15道小题，涉及的知识点的确增多了，但是很多题都是“送分”型的。我也不清楚小题和大题的时间分布，很有可能明天最终定稿的时候，把大题变为6道，10*4+15*2=70分，这种结构。
    填空和选择题涉及的都是基础概念与公式。计算证明题涵盖信道与信源、译码准则、循环码、BCH码、卷积码。没有任何超出讲授范围的试题。
    三、实验任务书待整理后再发信箱。实验报告12月中旬提交。  

    请大家注意序号与考场纪律，并请随时留意信箱，直至试前。

    网络实在不行，传不了附件，第8章ppt晚上传。

    此致，
敬礼！

身体健康！
万事如意！
哈尔滨工业大学
电子与信息工程学院
通信技术研究所
石硕 敬上 2015-11-24
TEL：86-451-86413513-8224 
M.P.：177-0364-7986；159-4519-1498。
</code></pre><h2 id="主题：2015-试卷形式及注意。"><a href="#主题：2015-试卷形式及注意。" class="headerlink" title="主题：2015 试卷形式及注意。"></a>主题：2015 试卷形式及注意。</h2><pre><code>大家好！

    这两天网络较差，这次试一试。第8章还是不成，今晚发。

    试卷形式：
    一、选择，2分*8题；
    二、填空，2分*7题；
    三、四，简单计算，各10分；
    五、证明，10分；
    六、电路设计，10分；
    七、八，计算，各15分。 

    请大家注意，在题签、答题册、草稿纸（可能是2张）都要写序号与姓名，并且都要交回，交回材料不足或信息不全会影响大家成绩。

    此致，
敬礼！

身体健康！
万事如意！
哈尔滨工业大学
电子与信息工程学院
通信技术研究所
石硕 敬上 2015-11-26
TEL：86-451-86413513-8224 
M.P.：177-0364-7986；159-4519-1498。
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;考试备战&quot;&gt;&lt;a href=&quot;#考试备战&quot; class=&quot;headerlink&quot; title=&quot;考试备战&quot;&gt;&lt;/a&gt;考试备战&lt;/h1&gt;&lt;h2 id=&quot;知识点整理&quot;&gt;&lt;a href=&quot;#知识点整理&quot; class=&quot;headerlink&quot; title=&quot;知识点整理&quot;&gt;&lt;/a&gt;知识点整理&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;一、信源熵与信道容量（计算，难度：中）
二、山农公式（计算，难度：易）？？？
三、马尔科夫（计算，难度：易）
四、霍夫曼（计算，难度：易）
五、译码准则（证明，难度：中）
六、线性分组码（1）（计算，难度：易）
七、线性分组码（2）（证明，难度：中-难）
八、循环码（计算，难度：中）
九、BCH码（计算，难度：难）
十、卷积码(电路)（综合，难度：中）
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="课程" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B/"/>
    
    
      <category term="信息论" scheme="http://yoursite.com/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>论文句子摘抄</title>
    <link href="http://yoursite.com/2016/11/07/%E8%AE%BA%E6%96%87%E5%8F%A5%E5%AD%90%E6%91%98%E6%8A%84/"/>
    <id>http://yoursite.com/2016/11/07/论文句子摘抄/</id>
    <published>2016-11-07T05:03:45.000Z</published>
    <updated>2016-11-07T05:04:39.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="英文论文好句子摘抄" scheme="http://yoursite.com/tags/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%BD%E5%8F%A5%E5%AD%90%E6%91%98%E6%8A%84/"/>
    
  </entry>
  
  <entry>
    <title>科研</title>
    <link href="http://yoursite.com/2016/11/06/%E7%A7%91%E7%A0%94/"/>
    <id>http://yoursite.com/2016/11/06/科研/</id>
    <published>2016-11-06T09:48:16.000Z</published>
    <updated>2016-11-06T13:07:20.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><a href="http://www.aclweb.org/anthology/index.html" target="_blank" rel="external">ACL Anthology</a></li>
<li><a href="https://www.google.com" target="_blank" rel="external">谷歌</a></li>
<li><a href="https://scholar.google.com" target="_blank" rel="external">谷歌学术</a></li>
</ul>
<a id="more"></a>
<h2 id="科研项目"><a href="#科研项目" class="headerlink" title="科研项目"></a>科研项目</h2><ul>
<li><a href="">ConceptNet</a></li>
<li><a href="">WordNet</a></li>
<li><a href="">LTP</a></li>
<li><a href="">BigCiLin</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/index.html&quot;&gt;ACL Anthology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.google.com&quot;&gt;谷歌&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://scholar.google.com&quot;&gt;谷歌学术&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="对科研有帮助的网站" scheme="http://yoursite.com/tags/%E5%AF%B9%E7%A7%91%E7%A0%94%E6%9C%89%E5%B8%AE%E5%8A%A9%E7%9A%84%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>兴趣</title>
    <link href="http://yoursite.com/2016/11/06/%E5%85%B4%E8%B6%A3/"/>
    <id>http://yoursite.com/2016/11/06/兴趣/</id>
    <published>2016-11-06T09:38:02.000Z</published>
    <updated>2016-11-06T13:08:03.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><a href="https://www.kaggle.com/" target="_blank" rel="external">kaggle</a></li>
<li><a href="https://tianchi.shuju.aliyun.com/competition/index.htm" target="_blank" rel="external">tianchi</a></li>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow</a></li>
<li><a href="https://www.zhihu.com" target="_blank" rel="external">zhihu</a></li>
<li><a href="http://weibo.com/u/2494805143" target="_blank" rel="external">weibo</a></li>
<li><a href="https://www.douban.com" target="_blank" rel="external">douban</a></li>
</ul>
<a id="more"></a>
<h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><ul>
<li><a href="http://python.freelycode.com" target="_blank" rel="external">python部落</a></li>
<li><a href="http://www.numpy.org" target="_blank" rel="external">numpy</a></li>
<li><a href="http://www.scipy.org" target="_blank" rel="external">Scipy</a></li>
<li><a href="http://pandas.pydata.org" target="_blank" rel="external">Pandas</a></li>
<li><a href="http://matplotlib.org" target="_blank" rel="external">Matplotlib</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tianchi.shuju.aliyun.com/competition/index.htm&quot;&gt;tianchi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com&quot;&gt;zhihu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://weibo.com/u/2494805143&quot;&gt;weibo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.douban.com&quot;&gt;douban&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="感兴趣的网站" scheme="http://yoursite.com/tags/%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>书籍</title>
    <link href="http://yoursite.com/2016/11/06/%E4%B9%A6%E7%B1%8D/"/>
    <id>http://yoursite.com/2016/11/06/书籍/</id>
    <published>2016-11-06T09:34:11.000Z</published>
    <updated>2016-11-25T03:14:59.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在线书籍"><a href="#在线书籍" class="headerlink" title="在线书籍"></a>在线书籍</h1><ul>
<li><a href="http://iissnan.com/progit/index.html" target="_blank" rel="external">progit</a></li>
<li><a href="http://www.deeplearningbook.org" target="_blank" rel="external">deeplearningbook</a></li>
<li><a href="http://www.scheme.com/tspl4/" target="_blank" rel="external">The Scheme Programming Language</a></li>
<li><a href="http://www.shido.info/lisp/idx_scm_e.html" target="_blank" rel="external">Yet Another Scheme Tutorial</a></li>
<li><a href="http://deathking.github.io/yast-cn" target="_blank" rel="external">Yet Another Scheme Tutorial中文版</a></li>
</ul>
<a id="more"></a>
<h1 id="我的书单"><a href="#我的书单" class="headerlink" title="我的书单"></a>我的书单</h1><h2 id="Kindle-Top-100人气小说第二期："><a href="#Kindle-Top-100人气小说第二期：" class="headerlink" title="Kindle Top 100人气小说第二期："></a>Kindle Top 100人气小说第二期：</h2><ul>
<li>三体</li>
<li>无声告白</li>
<li>古董局中局</li>
<li>火星救援</li>
<li>安柏志</li>
<li>岛上书店</li>
<li>古龙作品集</li>
<li>大秦帝国</li>
<li>刘慈欣最爱：阿瑟克拉克经典科幻：神的九十亿个名字，最后一个地球人，遥远地球之歌，地光</li>
<li>阿弥陀佛么么哒</li>
<li>余华长篇小说集：在细雨中呼喊，兄弟，许三观卖血记，或者</li>
<li>虚无的十字架</li>
<li>秘密：东野圭吾</li>
<li>乖，摸摸头</li>
<li>银河帝国：基地七部曲</li>
<li>权力巅峰的女人</li>
<li>我不知道该说些什么，关于死亡还是爱情</li>
<li>海伯利安四部曲</li>
<li>大江东去：全3册</li>
<li>生活与命运</li>
<li>地海传奇六部曲</li>
<li>太阳黑子</li>
<li>杰茨费拉德文集</li>
<li>移动迷宫：全三本</li>
<li>丈量世界</li>
<li>王小波文集：全十卷</li>
<li>伪装者</li>
<li>彷徨之刃</li>
<li>苏菲的世界</li>
<li>那片星空，那片海</li>
<li>永恒的终结</li>
<li>清明上河图密码</li>
<li>龙族全五册</li>
<li>剑桥简明金庸武侠史</li>
<li>云中歌</li>
<li>李自成</li>
<li>间谍课：上帝的拳头，暗杀名单</li>
<li>余罪</li>
<li>跟孩子一起学说话</li>
<li>韩寒MOOK7：我们从未陌生过</li>
<li>等待</li>
<li>锌皮娃娃兵</li>
<li>事件移民</li>
<li>人间失格</li>
<li>我的晃荡的青春</li>
<li>且将生活一饮而尽</li>
<li>悖论13</li>
<li>罪全书5</li>
<li>空中杀人现场</li>
<li>阴阳师典藏合集：5册</li>
<li>我是女兵，也是女人</li>
<li>亚特兰蒂斯：基因战争</li>
<li>一个人的朝圣1和2</li>
<li>暮光之城</li>
<li>和喜欢的一切在一起</li>
<li>华胥引</li>
<li>孔二狗的江湖</li>
<li>无人生还</li>
<li>忽然七日</li>
<li>冰与火之歌</li>
<li>长相思</li>
<li>苏丝黄的世界</li>
<li>那些我们没谈过的事</li>
<li>黄雀记</li>
<li>股票大作手回忆录</li>
<li>神探伽利略</li>
<li>后宫如懿传</li>
<li>我们夜里在美术馆谈恋爱</li>
<li>沉默的羔羊</li>
<li>牧羊少年奇幻之旅</li>
<li>曾国藩</li>
<li>李碧华经典小说集</li>
<li>忘川</li>
<li>二月河文集：康熙大帝</li>
<li>微宇宙的上帝</li>
<li>东野圭吾笑系列四部曲</li>
<li>教父</li>
<li>北平无战事</li>
<li>米奇-阿尔博姆系列套装</li>
<li>死亡通知单系列</li>
<li>我只是感和别人不一样</li>
<li>绑架游戏</li>
<li>精灵宝钻</li>
<li>当我足够爱你，才敢失去你</li>
<li>平生欢</li>
<li>面包匠的狂欢节</li>
<li>我还是想你妈妈</li>
<li>在不安的世界安静地活</li>
<li>致遗忘了我的你</li>
<li>簪中录</li>
<li>何所冬暖，何所夏凉</li>
<li>三嫁惹君心</li>
</ul>
<h2 id="Kindle-Top-100人气小说第一期："><a href="#Kindle-Top-100人气小说第一期：" class="headerlink" title="Kindle Top 100人气小说第一期："></a>Kindle Top 100人气小说第一期：</h2><ul>
<li>解忧杂货店</li>
<li>教父</li>
<li>银河帝国</li>
<li>从你的全世界路过</li>
<li>失乐园</li>
<li>红顶商人胡雪岩</li>
<li>白夜行</li>
<li>藏地密码</li>
<li>偷影子的人</li>
<li>追风筝的人</li>
<li>荆棘鸟</li>
<li>魔戒三部曲</li>
<li>第七天</li>
<li>世界上所有通话都是写给大人看的</li>
<li>再美也美不过想象</li>
<li>单恋</li>
<li>嫌疑人X的献身</li>
<li>无影灯</li>
<li>让我留在你身边</li>
<li>饥饿游戏</li>
<li>第十一根手指</li>
<li>安珀志</li>
<li>我是个算命先生</li>
<li>陆犯焉识</li>
<li>纳尼亚传奇</li>
<li>大清相国</li>
<li>白鹿原</li>
<li>情人</li>
<li>妈阁是座城</li>
<li>伊斯坦布尔假期</li>
<li>菲利普迪克科幻短篇小说集</li>
<li>了不起的盖茨比</li>
<li>悟空传</li>
<li>亲爱的生活</li>
<li>当我足够好，才会遇见你</li>
<li>比恐惧更强烈的情感</li>
<li>活着活着就老了</li>
<li>何以笙箫默</li>
<li>霍比特人</li>
<li>他方世界</li>
<li>1980年代的爱情</li>
<li>空间三部曲：沉寂的星球，皮尔兰德拉星，黑暗之劫</li>
<li>一个人的朝圣</li>
<li>一九八四</li>
<li>毛姆短篇小说集</li>
<li>消失的爱人</li>
<li>悉达多</li>
<li>逃离</li>
<li>朗读者</li>
<li>丰乳肥臀</li>
<li>青蛇</li>
<li>羊毛战记</li>
<li>大漠谣</li>
<li>福尔摩斯探案全集</li>
<li>掌舵</li>
<li>穆斯林的葬礼</li>
<li>假面饭店</li>
<li>起风了</li>
<li>三生三世</li>
<li>我也会爱上别人的</li>
<li>幻夜</li>
<li>生死疲劳</li>
<li>当我们谈论爱情时我们在谈论什么</li>
<li>无证之罪</li>
<li>廊桥遗梦</li>
<li>最美的时光</li>
<li>群山回唱</li>
<li>魔兽世界</li>
<li>华尔街之狼</li>
<li>反乌托邦三部曲：1984，美妙的新世界，我们</li>
<li>灿烂千阳</li>
<li>天堂向左，深圳往右</li>
<li>因为是医生</li>
<li>掌舵</li>
<li>基督山伯爵</li>
<li>画中情缘</li>
<li>狼图腾</li>
<li>少年PI的奇幻漂流</li>
<li>北回归线</li>
<li>星际迷航：红杉</li>
<li>偶发空缺</li>
<li>爱与黑暗的故事</li>
<li>大漠苍狼全集</li>
<li>X的悲剧</li>
<li>布登波洛克一家</li>
<li>分歧者</li>
<li>神经漫游者</li>
<li>别相信任何人</li>
<li>所有年轻人都将在黎明前死去</li>
<li>暗恋-橘生淮南</li>
<li>霸王别姬</li>
<li>海明威精选集</li>
<li>深渊上的火</li>
<li>安德的游戏</li>
<li>时间回旋</li>
<li>平行的宇宙</li>
<li>天意</li>
<li>中国通史</li>
<li>自卑与超越</li>
<li>跟任何人都聊得来</li>
<li>人间失格</li>
<li>人间喜剧</li>
<li>1984</li>
<li>钟形罩</li>
<li>蒙马特遗书</li>
<li>晚学盲言</li>
</ul>
<h2 id="素描"><a href="#素描" class="headerlink" title="素描"></a>素描</h2><ul>
<li>素描的诀窍，作者: 伯特·多德森 ，2005，2011</li>
<li>素描进阶教程，作者: 基蒙・尼克莱代斯 </li>
<li>人体素描，作者: 罗伯特.贝弗利.黑尔 </li>
</ul>
<h2 id="摄影构图"><a href="#摄影构图" class="headerlink" title="摄影构图"></a>摄影构图</h2><ul>
<li>摄影构图原理，作者: [美]Andreas Feininger </li>
<li>摄影构图学：作者: 【美】本·克来门茨 / 大卫·罗森菲尔德 </li>
<li>摄影构图与色彩设计，作者: [德]拉尔德·曼特 </li>
<li>摄影构图与图像语言，作者: 科拉•巴尼克 / 格奥尔格•巴尼克 </li>
<li>摄影构图的最佳选择，作者: （德）恩斯特・韦伯 </li>
<li>COMPOSITION摄影构图，作者: (美)大卫·普拉克尔 </li>
</ul>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>别闹了，费曼先生</li>
<li>红楼梦</li>
<li>小王子</li>
<li>斯通纳</li>
<li>旧制度与大革命</li>
<li>从0到1</li>
<li>如何阅读一本书</li>
<li>人类简史</li>
<li>人工智能的未来</li>
<li>最好的告别</li>
<li>佛教史</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;在线书籍&quot;&gt;&lt;a href=&quot;#在线书籍&quot; class=&quot;headerlink&quot; title=&quot;在线书籍&quot;&gt;&lt;/a&gt;在线书籍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://iissnan.com/progit/index.html&quot;&gt;progit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.deeplearningbook.org&quot;&gt;deeplearningbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.scheme.com/tspl4/&quot;&gt;The Scheme Programming Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.shido.info/lisp/idx_scm_e.html&quot;&gt;Yet Another Scheme Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://deathking.github.io/yast-cn&quot;&gt;Yet Another Scheme Tutorial中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="在线书籍" scheme="http://yoursite.com/tags/%E5%9C%A8%E7%BA%BF%E4%B9%A6%E7%B1%8D/"/>
    
  </entry>
  
  <entry>
    <title>知乎</title>
    <link href="http://yoursite.com/2016/11/06/%E7%9F%A5%E4%B9%8E/"/>
    <id>http://yoursite.com/2016/11/06/知乎/</id>
    <published>2016-11-06T09:31:00.000Z</published>
    <updated>2016-11-06T13:07:27.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><a href="https://www.zhihu.com/question/35758158" target="_blank" rel="external">multi-task深度神经网络如何实现，优化？</a></li>
</ul>
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/35758158&quot;&gt;multi-task深度神经网络如何实现，优化？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="知乎有价值问题" scheme="http://yoursite.com/tags/%E7%9F%A5%E4%B9%8E%E6%9C%89%E4%BB%B7%E5%80%BC%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>博主</title>
    <link href="http://yoursite.com/2016/11/06/%E5%8D%9A%E4%B8%BB/"/>
    <id>http://yoursite.com/2016/11/06/博主/</id>
    <published>2016-11-06T09:11:45.000Z</published>
    <updated>2016-12-07T07:59:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h2><ul>
<li><a href="http://2hwp.com/archives/" target="_blank" rel="external">wepon</a></li>
<li><a href="http://www.jeyzhang.com/archives/" target="_blank" rel="external">jeyzhang</a></li>
<li><a href="https://www.yunaitong.cn/" target="_blank" rel="external">yunaitong</a></li>
<li><a href="http://nanjunxiao.github.io/" target="_blank" rel="external">nanjunxiao强烈推荐</a></li>
<li><a href="https://handong1587.github.io/archives.html" target="_blank" rel="external">handong1587</a></li>
<li><a href="http://www.cnblogs.com/edwardbi/tag/深度学习/" target="_blank" rel="external">edwardbi tensorflow 深度学习</a></li>
<li><a href="http://www.jerryfu.net/" target="_blank" rel="external">jerryfu</a></li>
<li><a href="http://luuman.github.io/" target="_blank" rel="external">luuman</a></li>
<li><a href="http://www.cnblogs.com/yao62995/p/5773578.html" target="_blank" rel="external">yao62995 tensorflow源码</a></li>
<li><a href="http://www.cnblogs.com/sea-wind/p/4187588.html" target="_blank" rel="external">sea-wind ml 概率图模型 矩阵分析 指数分布族</a></li>
<li><a href="http://yanran.li/categories/" target="_blank" rel="external">yanran.li</a><a id="more"></a>
</li>
</ul>
<h2 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h2><ul>
<li><a href="https://sites.google.com/site/stephenclark609/publica" target="_blank" rel="external">stephenclark609</a></li>
<li><a href="http://karpathy.github.io/" target="_blank" rel="external">karpathy</a></li>
<li><a href="https://colah.github.io/" target="_blank" rel="external">colah</a></li>
<li><a href="https://handong1587.github.io/archives.html" target="_blank" rel="external">handong1587</a></li>
<li><a href="http://rakeshagrawal.org/" target="_blank" rel="external">rakeshagrawal</a></li>
<li><a href="http://ai.stanford.edu/~zayd/" target="_blank" rel="external">zayd stanford</a></li>
<li><a href="http://blog.aylien.com/" target="_blank" rel="external">aylien</a></li>
<li><a href="http://rakeshagrawal.org/" target="_blank" rel="external">rakeshagrawal</a></li>
<li><a href="http://homes.cs.washington.edu/~yejin/cse599.html" target="_blank" rel="external">yejin</a></li>
<li><a href="http://www.geekonomics10000.com/" target="_blank" rel="external">geekonomics10000</a></li>
</ul>
<h2 id="工具博客"><a href="#工具博客" class="headerlink" title="工具博客"></a>工具博客</h2><ul>
<li><a href="http://kateto.net/" target="_blank" rel="external">kateto.net/</a></li>
<li><a href="https://briatte.github.io/" target="_blank" rel="external">briatte</a></li>
<li><a href="http://x-wei.github.io/" target="_blank" rel="external">x-wei</a></li>
<li><a href="http://www.pythonchallenge.com/pc/return/5808.html" target="_blank" rel="external">pythonchallenge</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;中文&quot;&gt;&lt;a href=&quot;#中文&quot; class=&quot;headerlink&quot; title=&quot;中文&quot;&gt;&lt;/a&gt;中文&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://2hwp.com/archives/&quot;&gt;wepon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jeyzhang.com/archives/&quot;&gt;jeyzhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.yunaitong.cn/&quot;&gt;yunaitong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nanjunxiao.github.io/&quot;&gt;nanjunxiao强烈推荐&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://handong1587.github.io/archives.html&quot;&gt;handong1587&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/edwardbi/tag/深度学习/&quot;&gt;edwardbi tensorflow 深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jerryfu.net/&quot;&gt;jerryfu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://luuman.github.io/&quot;&gt;luuman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/yao62995/p/5773578.html&quot;&gt;yao62995 tensorflow源码&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/sea-wind/p/4187588.html&quot;&gt;sea-wind ml 概率图模型 矩阵分析 指数分布族&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://yanran.li/categories/&quot;&gt;yanran.li&lt;/a&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="棒棒的博主" scheme="http://yoursite.com/tags/%E6%A3%92%E6%A3%92%E7%9A%84%E5%8D%9A%E4%B8%BB/"/>
    
  </entry>
  
  <entry>
    <title>项目</title>
    <link href="http://yoursite.com/2016/11/06/%E9%A1%B9%E7%9B%AE/"/>
    <id>http://yoursite.com/2016/11/06/项目/</id>
    <published>2016-11-06T09:10:09.000Z</published>
    <updated>2016-11-17T02:18:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="软件包"><a href="#软件包" class="headerlink" title="软件包"></a>软件包</h1><ul>
<li><a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="external">xgboost doc</a></li>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">tensorflow</a></li>
</ul>
<h1 id="可视化软件"><a href="#可视化软件" class="headerlink" title="可视化软件"></a>可视化软件</h1><ul>
<li><a href="http://www.texmacs.org/" target="_blank" rel="external">texmacs</a></li>
</ul>
<h1 id="network可视化"><a href="#network可视化" class="headerlink" title="network可视化"></a>network可视化</h1><ul>
<li><a href="https://github.com/networkx/networkx" target="_blank" rel="external">networkx</a></li>
<li><a href="https://nodexl.codeplex.com/" target="_blank" rel="external">nodeXL for windows</a></li>
<li><a href="http://www.cytoscape.org/what_is_cytoscape.html" target="_blank" rel="external">cytoscape</a></li>
<li><a href="https://gephi.org/" target="_blank" rel="external">gephi</a></li>
<li><a href="http://schoolofdata.org/2014/08/20/4-network-visualisation-tools/" target="_blank" rel="external">4-network-visualisation-tools</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;软件包&quot;&gt;&lt;a href=&quot;#软件包&quot; class=&quot;headerlink&quot; title=&quot;软件包&quot;&gt;&lt;/a&gt;软件包&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/&quot; target=&quot;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="github好项目" scheme="http://yoursite.com/tags/github%E5%A5%BD%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>博客</title>
    <link href="http://yoursite.com/2016/11/06/%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2016/11/06/博客/</id>
    <published>2016-11-06T09:09:26.000Z</published>
    <updated>2016-11-17T02:18:12.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><a href="http://www.cnblogs.com/kaituorensheng/p/3569436.html" target="_blank" rel="external">Wordnet 与 Hownet 比较</a></li>
<li><a href="http://www.cnblogs.com/chenyadong/archive/2013/01/04/2844138.html" target="_blank" rel="external">《Using Cross-Entity Inference to Improve Event Extraction》读书笔记</a></li>
<li><a href="http://nanjunxiao.github.io/2015/08/05/GBM%E4%B9%8BGBRT%E6%80%BB%E7%BB%93/" target="_blank" rel="external">GBM之GBRT总结</a></li>
<li><a href="https://www.nervanasys.com/deep-reinforcement-learning-with-neon/" target="_blank" rel="external">Guest Post (Part II): Deep Reinforcement Learning with Neon</a></li>
<li><a href="http://x-wei.github.io/TeXmacs_intro.html" target="_blank" rel="external">学术文章写作利器: TeXmacs介绍</a></li>
</ul>
<a id="more"></a>
<ul>
<li><a href="https://www.nervanasys.com/openai/" target="_blank" rel="external">Blog Post (Part III): Deep Reinforcement Learning with OpenAI Gym</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="external">word2vec 中的数学原理详解</a></li>
<li><a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external">理解LSTM网络</a></li>
</ul>
<h1 id="微信文章"><a href="#微信文章" class="headerlink" title="微信文章"></a>微信文章</h1><ul>
<li><a href="">NAACL2016年度论文：无监督神经网络理解虚构人物关系</a></li>
<li><a href="">Facebook AI实验室最新论文：图像检测的无监督学习(下载)</a></li>
<li><a href="">【深度】LeCun解读Facebook人工智能最新进展：最大挑战是无监督学习</a></li>
<li><a href="">无监督学习：从基本概念到四种实现模型</a></li>
<li><a href="">Open AI 研究主管：实现无监督学习的最佳路径或是聚焦模型</a></li>
<li><a href="">深度 | 一篇文章带你进入无监督学习：从基本概念到四种实现模型（附论文）</a></li>
<li><a href="">【趋势】Yoshua Bengio: 机器的梦可以让我们实现无监督学习</a></li>
<li><a href="">深度思考｜人工智能、机器学习以及无监督学习的现在和未来</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/kaituorensheng/p/3569436.html&quot;&gt;Wordnet 与 Hownet 比较&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/chenyadong/archive/2013/01/04/2844138.html&quot;&gt;《Using Cross-Entity Inference to Improve Event Extraction》读书笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nanjunxiao.github.io/2015/08/05/GBM%E4%B9%8BGBRT%E6%80%BB%E7%BB%93/&quot;&gt;GBM之GBRT总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.nervanasys.com/deep-reinforcement-learning-with-neon/&quot;&gt;Guest Post (Part II): Deep Reinforcement Learning with Neon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://x-wei.github.io/TeXmacs_intro.html&quot;&gt;学术文章写作利器: TeXmacs介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="优秀博文" scheme="http://yoursite.com/tags/%E4%BC%98%E7%A7%80%E5%8D%9A%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>学者</title>
    <link href="http://yoursite.com/2016/11/06/%E5%AD%A6%E8%80%85/"/>
    <id>http://yoursite.com/2016/11/06/学者/</id>
    <published>2016-11-06T09:09:17.000Z</published>
    <updated>2016-12-15T04:17:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="国内"><a href="#国内" class="headerlink" title="国内"></a>国内</h1><ul>
<li><a href="">刘知远</a></li>
<li><a href="">张梅山</a></li>
<li><a href="">丁效</a></li>
<li><a href="">周志华</a></li>
<li><a href="">郭江</a></li>
<li><a href="">唐都钰</a></li>
<li><a href="http://playbigdata.com/batmanfly/" target="_blank" rel="external">赵鑫batmanfly</a></li>
<li><a href="">葛涛</a></li>
</ul>
<a id="more"></a>
<h1 id="Script-Learning"><a href="#Script-Learning" class="headerlink" title="Script Learning"></a>Script Learning</h1><ol>
<li>斯坦福大学</li>
</ol>
<ul>
<li><a href="http://cs.stanford.edu/people/nc/" target="_blank" rel="external">Nate Chambers</a></li>
<li><a href="http://web.stanford.edu/~jurafsky/" target="_blank" rel="external">Dan Jurafsky</a></li>
</ul>
<ol>
<li>德国萨尔大学 Saarland University</li>
</ol>
<ul>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=59" target="_blank" rel="external">Saarland Projects</a></li>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=2582" target="_blank" rel="external">Recourses</a></li>
<li><a href="http://www.coli.uni-saarland.de/~zarcone/page.php?id=index" target="_blank" rel="external">Alessandra Zarcone</a></li>
<li><a href="http://www.coli.uni-saarland.de/~ashutosh/" target="_blank" rel="external">Ashutosh Modi</a></li>
<li><a href="http://www.coli.uni-saarland.de/~pinkal/en/page.php?id=index" target="_blank" rel="external">Manfred Pinkal</a></li>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=52" target="_blank" rel="external">Members</a></li>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=1847" target="_blank" rel="external">Simon Ostermann</a></li>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=1606" target="_blank" rel="external">Lilian D. A. Wanzare</a></li>
<li><a href="http://www.sfb1102.uni-saarland.de/?page_id=1608" target="_blank" rel="external">Alessandra Zarcone</a></li>
<li><a href="http://www.coli.uni-saarland.de/~stth/" target="_blank" rel="external">Stefan Thater</a></li>
<li><p><a href="http://www.coli.uni-saarland.de/~vera/" target="_blank" rel="external">Vera Demberg</a></p>
</li>
<li><p><a href="">Tatjana Anikina</a></p>
</li>
<li><a href="">Alexander Koller</a></li>
</ul>
<ol>
<li>约翰霍普金斯大学</li>
</ol>
<ul>
<li><a href="http://www.clsp.jhu.edu/people/graduate-students/rachel-rudinger/" target="_blank" rel="external">Rachel Rudinger</a></li>
<li><a href="http://rudinger.github.io/" target="_blank" rel="external">Rachel Rudinger githubio</a></li>
<li><a href="https://scholar.google.com/citations?user=QKCHaHUAAAAJ&amp;hl=zh-CN" target="_blank" rel="external">Rachel Rudinger google citation</a></li>
<li><a href="https://www.linkedin.com/in/rachel-rudinger-21814820" target="_blank" rel="external">Rachel Rudinger linkedin</a></li>
<li><a href="https://github.com/rudinger" target="_blank" rel="external">Rachel Rudinger github</a></li>
<li><a href="http://dblp.uni-trier.de/pers/hd/r/Rudinger:Rachel" target="_blank" rel="external">Rachel Rudinger DBLP</a></li>
<li><a href="http://cs.jhu.edu/~ferraro/" target="_blank" rel="external">Francis Ferraro</a></li>
<li><a href="https://scholar.google.com/citations?user=UB5EoOEAAAAJ&amp;hl=en" target="_blank" rel="external">Francis Ferraro google citation</a></li>
<li><a href="http://www.cs.jhu.edu/~vandurme/" target="_blank" rel="external">Benjamin Van Durme</a></li>
</ul>
<ol>
<li>佐治亚理工学院</li>
</ol>
<ul>
<li><a href="http://boyangli.co/index.php" target="_blank" rel="external">boyangli</a></li>
<li><a href="">Stephen Lee-Urban</a></li>
<li><a href="">Darren Scott Appling</a></li>
<li><a href="">Mark O. Riedl</a></li>
<li><a href="">Fatima A. Boujarwah</a></li>
</ul>
<ol>
<li>德克萨斯大学，奥斯汀</li>
</ol>
<ul>
<li><a href="http://www.cs.utexas.edu/~pichotta/" target="_blank" rel="external">Karl Pichotta</a></li>
<li><a href="http://www.cs.utexas.edu/users/ai-lab/?mooney" target="_blank" rel="external">Raymond J. Mooney</a></li>
<li><a href="http://dblp.uni-trier.de/pers/hd/m/Mooney:Raymond_J=" target="_blank" rel="external">Mooney DBLP</a></li>
</ul>
<ol>
<li>俄勒冈州立大学</li>
</ol>
<ul>
<li><a href="https://www.linkedin.com/in/walker-orr-2871157" target="_blank" rel="external">Walker Orr linkedin</a></li>
<li><a href="http://research.engr.oregonstate.edu/dral/" target="_blank" rel="external">Deep Reading and Learning lab</a></li>
<li><a href="">Stephen Lee-Urban</a></li>
<li><a href="">Darren Scott Appling</a></li>
<li><a href="">Mark O. Riedl</a></li>
</ul>
<ol>
<li>英国爱丁堡大学</li>
</ol>
<ul>
<li><a href="">Lea Frermann</a></li>
<li><a href="">Neil McIntyre</a></li>
<li><a href="">Mirella Lapata</a></li>
</ul>
<ol>
<li>荷兰阿姆斯特丹大学</li>
</ol>
<ul>
<li><a href="">Ivan Titov</a></li>
</ul>
<ol>
<li>剑桥大学</li>
</ol>
<ul>
<li><a href="">Mark Granroth-Wilding</a></li>
<li><a href="">Stephen Clark</a></li>
</ul>
<ol>
<li>日本</li>
</ol>
<ul>
<li><a href="">Toshiaki Fujiki</a></li>
<li><a href="">Hidetsugu Nanba</a></li>
</ul>
<ol>
<li>比利时</li>
</ol>
<ul>
<li><a href="">Bram Jans</a></li>
<li><a href="">Ivan Vuli´c,Marie Francine Moens,KU Leuven Leuven</a></li>
<li><a href="">Marie Francine Moens</a></li>
<li><a href="">KU Leuven Leuven</a></li>
</ul>
<ol>
<li>美国科罗拉多</li>
</ol>
<ul>
<li><a href="">Steven Bethard</a></li>
</ul>
<h1 id="Extracting-Temporal-and-Causal-Relations-between-Events"><a href="#Extracting-Temporal-and-Causal-Relations-between-Events" class="headerlink" title="Extracting Temporal and Causal Relations between Events"></a>Extracting Temporal and Causal Relations between Events</h1><ul>
<li><a href="http://people.mpi-inf.mpg.de/~paramita/" target="_blank" rel="external">Paramita Mirza 刚毕业的博士，目前在德国</a></li>
<li><a href="http://paramitamirza.com/" target="_blank" rel="external">Paramita Mirza 的个人博客</a></li>
<li><a href="http://dh.fbk.eu/people/profile/satonelli" target="_blank" rel="external">Sara Tonelli DH.FBK，意大利的科研机构 参加过NewsReader</a></li>
</ul>
<h1 id="国外"><a href="#国外" class="headerlink" title="国外"></a>国外</h1><ul>
<li><a href="">Heng Ji</a></li>
<li><a href="">张越</a></li>
<li><a href="">韩家炜</a></li>
<li><a href="https://people.eecs.berkeley.edu/~jordan/publications.html" target="_blank" rel="external">jordan</a></li>
<li><a href="http://l2r.cs.illinois.edu/tutorials.html" target="_blank" rel="external">Dan Roth</a></li>
<li><a href="http://kevinchai.net/datasets" target="_blank" rel="external">kevinchai</a></li>
<li><a href="https://sites.google.com/site/yangdingqi/home" target="_blank" rel="external">yangdingqi</a></li>
<li><a href="http://sebastianruder.com/publications/" target="_blank" rel="external">Sebastian Ruder</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/clucas2/" target="_blank" rel="external">Christopher G Lucas 因果推断</a></li>
</ul>
<h1 id="微软"><a href="#微软" class="headerlink" title="微软"></a>微软</h1><ul>
<li><a href="http://research.microsoft.com/en-us/um/people/horvitz/" target="_blank" rel="external">horvitz</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;国内&quot;&gt;&lt;a href=&quot;#国内&quot; class=&quot;headerlink&quot; title=&quot;国内&quot;&gt;&lt;/a&gt;国内&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;刘知远&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;张梅山&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;丁效&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;周志华&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;郭江&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;唐都钰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://playbigdata.com/batmanfly/&quot;&gt;赵鑫batmanfly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;葛涛&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="关注的学者" scheme="http://yoursite.com/tags/%E5%85%B3%E6%B3%A8%E7%9A%84%E5%AD%A6%E8%80%85/"/>
    
  </entry>
  
  <entry>
    <title>会议</title>
    <link href="http://yoursite.com/2016/11/06/%E4%BC%9A%E8%AE%AE/"/>
    <id>http://yoursite.com/2016/11/06/会议/</id>
    <published>2016-11-06T09:09:01.000Z</published>
    <updated>2016-12-13T02:50:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="其他会议信息"><a href="#其他会议信息" class="headerlink" title="其他会议信息"></a>其他会议信息</h1><ul>
<li><a href="https://openreview.net/" target="_blank" rel="external">openreview</a><br>  公开审稿的几个会议</li>
</ul>
<h1 id="Conference-From-chenyadong"><a href="#Conference-From-chenyadong" class="headerlink" title="Conference From chenyadong"></a>Conference From <a href="www.cnblogs.com/chenyadong">chenyadong</a></h1><h2 id="A类："><a href="#A类：" class="headerlink" title="A类："></a>A类：</h2><ul>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">ACL(ACL) Meeting of the Association for Computational Linguistics </a></li>
<li><a href="">IJCAI(AAAI) International Joint Conference on Artificial Intelligence 国际人工智能联合会议</a></li>
<li><a href="http://www.aaai.org/Library/IJCAI/ijcai-library.php" target="_blank" rel="external">IJCAI</a></li>
<li><a href="http://www.aaai.org/Library/AAAI/aaai-library.php" target="_blank" rel="external">AAAI(AAAI) National Conference on Artificial Intelligence</a> <a href="http://www.aaai.org/Library/conferences-library.php" target="_blank" rel="external">AAAI所有会议下载地址</a></li>
<li><a href="">SIGIR(ACM) International ACM SIGIR Conference on Research and Development in Information Retrieval</a></li>
<li><a href="">SIGKDD(ACM) Workshop on Large-Scale Parallel KDD Systems</a></li>
<li><a href="">WWW(ACM) International World Wide Web Conferences</a></li>
<li><a href="http://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" rel="external">ICDE(IEEE) International Conference on Data Engineering</a></li>
</ul>
<a id="more"></a>
<h2 id="B类"><a href="#B类" class="headerlink" title="B类"></a>B类</h2><ul>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">EMNLP(ACL) Empirical Methods in Natural Language Processing</a></li>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">NAACL(ACL) North American Chapter of the Association for Computational Linguistics</a></li>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">COLING(ACL) International Conference on Computational Linguistics</a></li>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">EACL(ACL) Conference of the European Chapter of the Association for Computational Linguistics</a></li>
<li><a href="">CIKM(ACM) Conference on Information and Knowledge Management</a></li>
<li><a href="http://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" rel="external">ICDM(IEEE) International Conference on Data Mining</a></li>
</ul>
<h2 id="C类"><a href="#C类" class="headerlink" title="C类"></a>C类</h2><ul>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">IJCNLP(ACL)</a></li>
</ul>
<h2 id="D类"><a href="#D类" class="headerlink" title="D类"></a>D类</h2><ul>
<li><a href="http://www.aclweb.org/anthology-new/" target="_blank" rel="external">PACLIC(ACL)</a></li>
<li><a href="http://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" rel="external">IALP(IEEE)</a></li>
<li><a href="">CLSW 汉语词汇语义学研讨会 Chinese Lexical Semantics Workshop</a></li>
<li><a href="">CCIR 全国信息检索学术会议</a></li>
<li><a href="http://springer.lib.tsinghua.edu.cn/" target="_blank" rel="external">CCL Conference of Computational Linguistics ?Constraints in Computational Logics</a></li>
<li><a href="">YCCL Youth Conference of Computational Linguistics 青年计算语言学会议</a></li>
</ul>
<h1 id="Conference-From-GeTao"><a href="#Conference-From-GeTao" class="headerlink" title="Conference From GeTao"></a>Conference From GeTao</h1><p>Here listed below are prestigious conferences related to my research field – Natural Language Processing (NLP), Data Mining (DM), Machine Learning (ML) and Information Retrieval (IR).</p>
<h2 id="A-Class"><a href="#A-Class" class="headerlink" title="A Class"></a>A Class</h2><p>SIGKDD: The best data mining conference.<br>ACL: The best NLP/CL conference.<br>SIGIR: The best Information Retrieval (IR) conference.<br>WWW: The best conference on Web<br>ICML: Top machine learning conference.<br>NIPS: Top machine learning conference.<br>VLDB: Top conference on database.<br>SIGMOD: Top conference on database.<br>IJCAI: Top AI conference.<br>AAAI: Top AI conference.</p>
<h2 id="B-Class"><a href="#B-Class" class="headerlink" title="B Class"></a>B Class</h2><p>CIKM: Famous conference on database and knowledge management<br>EMNLP: Famous conference on natural language processing<br>Recsys: Famous conference on recommender systems.<br>ECML/PKDD: Famous conference on machine learning and data mining<br>ICDM: Famous conference on data mining.<br>NAACL: Famous conference on natural language processing<br>WSDM: Famous conference on IR and DM<br>COLING: Famous conference on NLP/CL.<br>SDM: Famous conference on data mining</p>
<h2 id="C-Class"><a href="#C-Class" class="headerlink" title="C Class"></a>C Class</h2><p>CoNLL: Good conference on natural language processing<br>PAKDD: Good conference on data mining<br>IJCNLP: Good conference on natural language processing<br>WISE: Good conference on Web<br>ECIR: Good conference on IR<br>AIRS: Good conference on IR in Pacific Asia<br>ACML: Good conference on Machine learning in Asia<br>APWeb: Good conference on Web in Pacific Asia<br>WAIM: Good conference on web information management</p>
<h2 id="Chinese-Conference"><a href="#Chinese-Conference" class="headerlink" title="Chinese Conference"></a>Chinese Conference</h2><p>NLPCC: Local NLP conference in China<br>CCL: Local NLP conference in China<br>CCIR: Local IR conference in China<br>CCML: Local ML conference in China</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;其他会议信息&quot;&gt;&lt;a href=&quot;#其他会议信息&quot; class=&quot;headerlink&quot; title=&quot;其他会议信息&quot;&gt;&lt;/a&gt;其他会议信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://openreview.net/&quot;&gt;openreview&lt;/a&gt;&lt;br&gt;  公开审稿的几个会议&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Conference-From-chenyadong&quot;&gt;&lt;a href=&quot;#Conference-From-chenyadong&quot; class=&quot;headerlink&quot; title=&quot;Conference From chenyadong&quot;&gt;&lt;/a&gt;Conference From &lt;a href=&quot;www.cnblogs.com/chenyadong&quot;&gt;chenyadong&lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;A类：&quot;&gt;&lt;a href=&quot;#A类：&quot; class=&quot;headerlink&quot; title=&quot;A类：&quot;&gt;&lt;/a&gt;A类：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology-new/&quot;&gt;ACL(ACL) Meeting of the Association for Computational Linguistics &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;IJCAI(AAAI) International Joint Conference on Artificial Intelligence 国际人工智能联合会议&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aaai.org/Library/IJCAI/ijcai-library.php&quot;&gt;IJCAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aaai.org/Library/AAAI/aaai-library.php&quot;&gt;AAAI(AAAI) National Conference on Artificial Intelligence&lt;/a&gt; &lt;a href=&quot;http://www.aaai.org/Library/conferences-library.php&quot;&gt;AAAI所有会议下载地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;SIGIR(ACM) International ACM SIGIR Conference on Research and Development in Information Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;SIGKDD(ACM) Workshop on Large-Scale Parallel KDD Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;WWW(ACM) International World Wide Web Conferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/Xplore/home.jsp&quot;&gt;ICDE(IEEE) International Conference on Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="信息整理" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%95%B4%E7%90%86/"/>
    
    
      <category term="我关注的会议" scheme="http://yoursite.com/tags/%E6%88%91%E5%85%B3%E6%B3%A8%E7%9A%84%E4%BC%9A%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2016/11/06/hello-world/"/>
    <id>http://yoursite.com/2016/11/06/hello-world/</id>
    <published>2016-11-06T02:50:38.000Z</published>
    <updated>2016-11-06T06:25:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h2 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h2 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h2 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h2 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>网页-微信文章-科普文章阅读笔记</title>
    <link href="http://yoursite.com/2016/04/02/%E7%BD%91%E9%A1%B5%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2016/04/02/网页文章阅读笔记/</id>
    <published>2016-04-02T14:20:41.000Z</published>
    <updated>2016-11-06T06:46:29.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li><p>机器学习那些事，刘知远</p>
<p> 得到的经验就是：<br> 特征工程很重要：特征选择，试错<br> 训练多个不同模型集成很重要：bagging，boosting，stacking</p>
<a id="more"></a>
</li>
<li><p>深度学习：推动NLP领域发展的新引擎</p>
<p> Word Embedding<br> 引入词的关系<br> 最常见的思路就是用Dependency Parser，把抽取出来的Relation作为词的Context。<br> 改进Bag of Words<br> 有人认为词（Word）的粒度也太大，可以到Character级别的，或者Morpheme级别的。<br> 外部资源和知识库<br> Word2vec只使用了词的上下文的共现，没有使用外部的资源如词典知识库等，因此也有不少工作对此进行改进。</p>
<p> RNN/LSTM/CNN<br> 最近CNN相关的改进模型也被用于NLP领域。今年的ACL上有很多RNN/LSTM/CNN用来做机器翻译（Machine Translation）、语义角色标注（Sematic Role Labeling）等。</p>
</li>
</ol>
<pre><code>Multi-model Deep Learning
这是当下的一个热门，不只考虑文本，同时也考虑图像，比如给图片生成标题（Caption）。当然这和传统的NLP任务不太一样，但这是一个非常有趣的方向，有点像小朋友学习看图说话。

Reasoning, Attention and Memory
前面说RNN/LSTM是试图模拟人类大脑的记忆机制，但除了记忆之外，Attention也是非常有用的机制。

Attention
最早Attention是在《Recurrent Models of Visual Attention》这篇文章提出来的。关于Attention，Google DeepMind的一篇论文《DRAW：A Recurrent Neural Network For Image》有一段非常好的解释。Attention除了模仿人类视觉系统的认知过程，还可以减少计算量，因为它排除了不关心的内容。而传统的模型如CNN，其计算复杂度就非常高。另外除了计算减少的好处之外，有选择地把计算资源（注意力）放在关键的地方而不是其它（可能干扰）的地方，还有可能提高识别准确率。就像一个人注意力很分散，哪都想看，反而哪都看不清楚。

Attention最早是在视觉领域应用，当然很快就扩展到NLP和Speech。
用来做机器翻译：Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. 2015. In Proceedings of ICLR.

做Summary：Alexander M. Rush, Sumit Chopra, Jason Weston. A Neural Attention Model for Sentence Summarization. 2015. In Proceedings of EMNLP.

Word Embedding: Not All Contexts Are Created Equal: Better Word Representations with Variable Attention. 2015. In Proceedings of EMNLP.

Speech领域：Attention-Based Models for Speech Recognition. 

其它的应用，比如Multimodel，Image的Caption生成： Attend and Tell: Neural Image Caption Generation with Visual Attention. Teaching Machines to Read and Comprehend. 2015. In Proceedings of NIPS.

前面最早的Attention Model是不可导的，只能用强化学习来优化，也被叫做Hard Attention，也就是把注意力集中在离散的区域；后来也有Soft的Attention，也就是在所有的区域都有Attention，但是连续分布的。Soft的好处是可导，因此可以用梯度下降这样的方法来训练模型，和传统的神经网络更加接近。但坏处就是因为所有的区域都有值（非零），这就增加了计算量。用个不恰当的比方，一个是稀疏矩阵，一个是稠密的矩阵，计算量当然差别就很大。

也有一些工作尝试融合Soft和Hard Attention的优点。

Memory的扩展
前面说到RNN，如LSTM，有Memory（记忆），很多模型对此也进行了拓展。

比如Neural Turing Machine (Neural Turing Machines. Alex Graves, Greg Wayne, Ivo Danihelka. arXiv Pre-Print, 2014)，NTM用一句话描述就是有外部存储的神经网络。

Language的复杂性
深度学习的一个方向Representation Learning其实就是有这个想法，不过目前更多关注的是一些具体任务的Feature的表示。更多是在Image和Speech领域，用在Language的较少，Word2vec等也可以看成表示概念的方式，不过这种向量的表示太过简单且没有结构化。更少有工作考虑用神经网络怎么表示人类已有的复杂知识。现在的知识表示还是以几十年前基于符号的形式逻辑的为主。

我们现在甚至有很多结构化的数据，比如企业数据库、维基百科的、Google的Freebase以及内部的Knowledge Graph。但目前都是用人类习惯的表示方式，比如三元组、图或者实体关系。但这样的表示方式是高层的抽象的，大脑里的神经元似乎不能处理，因此现在的Deep Neural Network很难整合已有的这些知识库进行推理等更有用的事情。

总结
从上面的分析我们大致可以看到最近NLP的发展趋势：深度神经网络尤其是RNN的改进，模拟人脑的Attention和Memory，更加结构化的Word Embedding或者说Knowledge Representation。
</code></pre><ol>
<li><p>sed百度百科阅读学习</p>
<p> sed -n ‘s/ID/&amp;id/p’ true.csv |more<br> sed /100/d true.csv |wc -l</p>
</li>
<li><p>如何判断一篇论文是否被SCI收录？</p>
<p> 有人说IDS number就是SCI检索号，而也有很多人说SCI检索号是UT ISI号(15位)。不知谁的说法对？认为是后一种的人更多。<br> IDS 在sci数据库中是这样的解释的：Thomson Reuters Document Solution® 编号。此号码是识别期刊和期号的唯一编号，用于订阅 Document Solution 中的文献的全文。也就是说该号是论文发表刊和所在期的编号。由此可知同一期的所有的论文IDS号是一样的。UT ISI号是Unique Article Identifier，是文章的唯一识别符。<br> 规律：<br> 看ids号<br> 数字在前  字母在后 SCI<br> 字母在前  数字在后  ISTP</p>
<p> “SCI、EI和ISTP收录号的查询方法（总结）”：<br> <a href="http://emuch.net/bbs/viewthread.php?tid=660421" target="_blank" rel="external">http://emuch.net/bbs/viewthread.php?tid=660421</a><br> “GA  BHE43 ”这栏里的“BHE43”字母在前数字在后就是ISTP检索，如果数字在前字母在后就是SCI检索。当然要注意一个特殊的情况，有些会议文章被期刊收录后，即使该期刊是SCI检索，查询检索时“GA  BHE43 ”这栏里的“BHE43”情形还是和ISTP检索类似，即字母在前数字在后。</p>
</li>
</ol>
<ol>
<li><p>原问题：如何查询一篇文献的影响因子</p>
<p> 亲。文献是没有影响因子的。有影响因子的那个是期刊。你直接查到期刊的主页，上面会有影响因子的。或者期刊名加上impact factor直接上Google上搜就能搜到。</p>
<p> 影响因子（Impact Factor,IF)是美国ISI（科学信息研究所）的JCR(期刊引证报告）中的一项数据。<br>   即某期刊前两年发表的论文在统计当年的被引用总次数除以该期刊在前两年内发表的论文总数。这是一个国际上通行的期刊评价指标。<br> 意义：该指标是相对统计值，可克服大小期刊由于载文量不同所带来的偏差。一般来说，影响因子越大，其学术影响力也越大。<br> 影响因子查询系统<br> <a href="http://emuch.net/sciif/" target="_blank" rel="external">http://emuch.net/sciif/</a><br> <a href="http://www.proteomics.com.cn/sci-if/" target="_blank" rel="external">http://www.proteomics.com.cn/sci-if/</a></p>
</li>
<li><p>Michael Jordan推荐的机器学习书籍 </p>
<p> “I now tend to add some books that dig still further into foundational topics. In particular, I recommend A. Tsybakov’s book “Introduction to Nonparametric Estimation” as a very readable source for the tools for obtaining lower bounds on estimators, and Y. Nesterov’s very readable “Introductory Lectures on Convex Optimization” as a way to start to understand lower bounds in optimization. I also recommend A. van der Vaart’s “Asymptotic Statistics”, a book that we often teach from at Berkeley, as a book that shows how many ideas in inference (M estimation—which includes maximum likelihood and empirical risk minimization—the bootstrap, semiparametrics, etc) repose on top of empirical process theory. I’d also include B. Efron’s “Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction”, as a thought-provoking book”<br> Introduction to Nonparametric Estimation<br> Introductory Lectures on Convex Optimization<br> Asymptotic Statistics<br> Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction</p>
<p> Tsybakov’s book is available online at Springer if your University has access to it: <a href="http://link.springer.com/book/10.1007%2Fb13794" target="_blank" rel="external">http://link.springer.com/book/10.1007%2Fb13794</a></p>
<p> Nesterov’s books is also available: <a href="http://link.springer.com/book/10.1007%2F978-1-4419-8853-9" target="_blank" rel="external">http://link.springer.com/book/10.1007%2F978-1-4419-8853-9</a></p>
<p> Thanks a lot! BTW, I gathered your recommendations on Goodreads: <a href="https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list" target="_blank" rel="external">https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list</a></p>
<p> In this post, I would like to blend together recommendations from academic and industry researchers: <a href="http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html" target="_blank" rel="external">http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html</a></p>
<p> Other comprehensive list of video lectures and some books. <a href="http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html" target="_blank" rel="external">http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html</a></p>
</li>
<li><p>SCI论文发表过程分析</p>
<p> 论文写作<br> 论文定位<br> 选定期刊<br> 在线投稿<br> 文章审理<br> 文章接收</p>
</li>
<li><p>搜索需要一场变革</p>
<p> 在万维网诞生20周年之际,奥伦·艾齐厄尼(OrenEtzioni)号召研究者开始思考比关键 词框更好的互联网搜索方式。<br> 互联网搜索正处于从简单的文档检索走向问题 回答深刻变革的风口浪尖上。用户在进行搜索时真正想要的并<br> 不是一串长长的包含关键词的文档列 表,而是他们所提问题的直接答案。</p>
<p> 由网页 构成的海量语料库是高度冗余的,也就是说一个断 言会以不同形式被多次表达。这样一来,当系统从 独立撰写的不同句子中多次抽取出相同的断言时, 它以此推导出一个正确事实的可能性便得到了成倍的提高。</p>
<p> 开放式信息抽取或许能够从实质上扩大这些工 具的使用范围。我们系统的开源代码可以通过http:// go.nature.com/ei3p4f获取。</p>
<p> 信息抽取不仅需要能够推 导由动词表达的关系,还要能够推导由名词或形容 词表达的关系。同时,信息往往要通过其<br> 来源、目 的和上下文才能充分体现,因此系统需要建立检测 这些因素或者其他更微妙因素的能力。最后,信息 抽取方法<br> 还需要扩展到其他语言中去,这也将面临 各种特殊的挑战。</p>
<p> 实现从信息检索到问答系统的范式转变所遇到的 最大障碍似乎是我们令人惊异地缺乏某种雄心和想象力。</p>
<p> 大多数自然语言处理研究聚焦于有限的几个任务 上。例如,它们关注如何分析句子的句法结构,却很 少关注如何揭示<br> 它们的意义;由于对人工标注数据的依赖,其方法无法适应大规模文档集合和任意主题的文本;所提出算法的计算<br> 花销也可能是随着数据的规模呈指数增长的。</p>
<p> 无论是检索文献的科学家,还是迅速增长的小屏幕手机用户(需要从互联网中撷取简洁回答),均将 受惠于通用<br> 的问答系统。没有它,我们将会冒被淹没 于信息汪洋大海之中的危险。</p>
</li>
<li><p>特征工程方法</p>
<p> 特征提取<br> 特征构造<br> 特征工程</p>
</li>
<li><p>kaggle特点</p>
</li>
<li><p>计算机科学的发展趋势，刘知远</p>
</li>
<li><p>复旦大学，GDM团队，肖仰化，崔万云（博三），图数据管理研究室</p>
<p>页面上有很多十分有用的信息，学生都很厉害！！<br>verb pattern demo，AAAI2016，崔万云</p>
</li>
<li><p>senticNet 网页阅读-信息丰富</p>
<p>Sentic Team-Projects:</p>
<p>KNOWLEDGE REPRESENTATION<br>SUBJECTIVITY DETECTION<br>MULTIMODAL SENTIMENT ANALYSIS<br>MULTILINGUAL SENTIMENT ANALYSIS<br>AUTOMATIC SPEECH RECOGNITION<br>STOCK MARKET PREDICTION</p>
</li>
<li><p>了解各个大型知识库的情况</p>
</li>
<li><p>降低知识图谱的构造成本 文因互联 鲍捷</p>
<p>轻重型知识项目随着成本提升，回报的提升</p>
<p>降低成本：</p>
<pre><code>依托成熟技术：知识表示，提取，存储，检索，人机交互（每个阶段都涉及到一定的成本）
迭代构建：data-idea-code
</code></pre><p>降低成本的核心是人：知识是写给人读的,只是碰巧能被机器执行</p>
</li>
<li><p>李飞飞做研究的建议</p>
<p>important problem (inspiring idea) + solid and novel theory + convincing and analytical experiments + good writing = seminal research + excellent paper.</p>
<p>重要的问题 + 坚固而新颖的理论 + 令人信服的分析实验 + 一流的写作 = 开创性的研究 + 出色的论文</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;&lt;p&gt;机器学习那些事，刘知远&lt;/p&gt;
&lt;p&gt; 得到的经验就是：&lt;br&gt; 特征工程很重要：特征选择，试错&lt;br&gt; 训练多个不同模型集成很重要：bagging，boosting，stacking&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记</title>
    <link href="http://yoursite.com/2016/01/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2016/01/14/论文阅读笔记/</id>
    <published>2016-01-14T02:08:19.000Z</published>
    <updated>2016-11-06T06:40:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习相关论文"><a href="#深度学习相关论文" class="headerlink" title="深度学习相关论文"></a>深度学习相关论文</h1><ol>
<li>机器学习系统，隐藏多少技术债？Hidden Technical Debt in Machine Learning Systems</li>
<li>Mastering the game of Go with deep neural networks and tree search_nature</li>
<li>Human-level control through deep reinforcement</li>
<li>Teaching Machines to Read and Comprehend</li>
</ol>
<a id="more"></a>
<h1 id="其他论文"><a href="#其他论文" class="headerlink" title="其他论文"></a>其他论文</h1><ol>
<li>Personalized Microtopic Recommendation on Microblogs,李洋，TIST</li>
<li>GraphX: Graph Processing in a Distributed Dataflow Framework</li>
<li>An Experimental Comparison of Pregel-like Graph Processing Systems</li>
</ol>
<h1 id="事理图谱写作相关思考"><a href="#事理图谱写作相关思考" class="headerlink" title="事理图谱写作相关思考"></a>事理图谱写作相关思考</h1><ul>
<li>Title : Another Commonsense Knowledge Base: EventNet</li>
<li>构建事理图谱需要解决的问题：</li>
<li>如何精确定义事件顺承关系，事件因果关系？</li>
<li>如何检测两个事件之间是否有关系？如果有关系，是属于顺承还是属于因果？</li>
<li>如何判断多个事件之间的顺承，因果依赖次序？</li>
<li>事理图谱的应用</li>
</ul>
<h1 id="事理图谱-EventNet-调研工作："><a href="#事理图谱-EventNet-调研工作：" class="headerlink" title="事理图谱(EventNet)调研工作："></a>事理图谱(EventNet)调研工作：</h1><ol>
<li><p>Diamonds in the Rough: Event Extraction from Imperfect Microblog Data  2015 NAACL</p>
<p> 地震事件抽取，CRF序列标注模型，20个事件参数</p>
<p> 为了克服微博数据的不准确和短文本造成的模棱两可特点(会给评估和系统开发造成困难），使用了两个方法:approximate(模糊匹配) distant supervision和feature aggregation（聚合多条微博数据的特征）来解决这两个问题。</p>
<p> 公开了地震数据集，来自维基百科和tweet.</p>
<p> 提出了一个新颖的评估方法（Lenient evaluation），更适合于这种不十分精确的实验结果评估。</p>
<p> distant supervision方法用于自动构造训练数据,不用手工标注训练数据，值得注意这种方法的使用。</p>
<p> 使用的是非结构化，信息繁杂的微博数据，而不是干净整洁的新闻报道数据。</p>
<p> 当时这篇论文看的不仔细，CRF具体如何用于实验当中，还得细看。</p>
</li>
<li><p>An Overview of Event Extraction from Text 2011<br> 2011年以前的事件抽取综述</p>
<p> 主要介绍了3类方法：数据驱动的方法，基于专家知识的模板匹配方法，杂交方法</p>
<p> 表格列出了论文参考文献中的方法，比较了各类方法的优缺点。</p>
<p> 当时本文都是基于干净的在线新闻文本做事件抽取，很少在微博这种短小不精确的文本上做事件抽取。 </p>
</li>
<li><p>Cross-document Event Extraction and Tracking-Task, Evaluation, Techniques and Challenges_RANLP2009</p>
<p> 跨文档的事件链条构建，以人名为中心词，跨文档事件聚合，融入背景知识的时间参数纠正，并按照时间排序各个事件。</p>
<p> 介绍了ACE任务中的名词术语和工作目标。提出了跨文档事件抽取的任务，融入了背景知识（相关文档和维基百科数据）来纠正事件参数（时间），并且提出了新的评价方法（Browshing cost，tempotal correlation），最后构建了一个以单文档事件抽取为基础的跨文档事件抽取系统。</p>
<p> 需要详细了解ACE任务：Automatic Content Extraction, 引用量最高的论文。</p>
</li>
<li><p>SenticNet 3: A Common and Common-Sense Knowledge Base for Cognition-Driven Sentiment Analysis 2014 AAAI</p>
<p> 是一个情感极性分析，观点挖掘的知识库。结合预处理，语义分析，观点目标词检测，能够组成一个完整的认知驱动的观点挖掘系统。</p>
<p> 现在有三个版本，公开了数据资源，用xml格式存储，数据库支持。</p>
<p> energy flow能量子流动，情感分类模型：Hourglass of Emotions，RDF－XML数据库存储，SBoCs。</p>
<p> 情感极性分类：</p>
<pre><code>1. keyword spotting
2. lexical affinity
3. statistical methods
4. concept-level approaches
</code></pre><p> 公共知识库：<br> DBPedia,Freebase,YAGO,WiKipedia,NELL,Probase.</p>
<p> 常识知识库：<br> Cyc,Open Mind Commom Sense(OMCS),ConceptNet,WordNet,WNA.</p>
<p> UK－national health service(NHS),PatientOpinion,英国公共医疗服务评价。</p>
</li>
<li><p>Towards a Chinese Common and Common Sense Knowledge Base for Sentiment Analysis LNAI 2012</p>
<p> 先分析了互联网上中文使用者和数据量将超越英文，然后说了构建中文情感分析知识库的必要性。</p>
<p> 混合Probase and ConceptNet来构建情感分析知识库，然后运用机器翻译方法（模糊匹配）将其翻译为中文。<br> 短文</p>
<p> 没有做evaluation。Conclusion写的很好。</p>
</li>
<li><p>Isanette: A Common and Common Sense Knowledge Base for Opinion Mining 2011 ICDM_wordshop</p>
<p> 混合Probase and ConceptNet来构建情感分析(观点挖掘)知识库，并解决了两个问题：1，多个词表示同一个意思（这个很有启发，事理图谱也会遇到同样的问题）2，降低数据的稀疏性，并用降维技术检测了含有的否定形式。</p>
<p> 最终知识库的形式：</p>
<pre><code>a matrix 340,000 × 200,000 whose rows are instances such as ‘birthday party’ and ‘china’, whose columns are concepts like ‘special occasion’ and ‘country’, and whose values indicate truth values of assertions.
</code></pre><p> 然后利用该知识库做reasoning和opinion mining：</p>
<p> reasoning：1将知识库用向量空间进行表示（Vector Space Representation），2进行<br> 语义聚类（Semantic Clustering）</p>
<p> opinion mining：1观点挖掘引擎（Opinion Mining Engine），2评价（Evaluation）</p>
<p> 结论：混合Probase and ConceptNet构建了开放领域的知识库，可以有效进行reasoning，进而完成观点挖掘，<br> 情感分析，文本自动分类等任务。</p>
<p> 好像没有开放资源：Isanette。<br> 利用该知识库做reasoning和opinion mining的部分有待细读。</p>
</li>
<li><p>A Survey on Truth Discovery SIGKDD,韩家炜</p>
<p> 有待精读。</p>
<p> 任务定义：<br> principle：<br> input：<br> output：<br> data：</p>
</li>
<li><p>中文社交媒体谣言统计语义分析_rumor2015，中国科学，刘知远，张乐，涂存超</p>
<p> 微博谣言影响力，产生与消亡的特点，谣言分类，时序分析，自动辟谣框架（贝叶斯公式，专家发现）</p>
<p> 具体专家发现方法见2012年的草考文献17</p>
</li>
<li><p>知识表示学习研究进展 刘知远，计算机研究与发展，2015</p>
<p> 知识表示学习简介：基本概念（独热表示，大量相关研究），理论基础（模拟人类大脑，多个突触状态表示复杂知识），典型应用，主要优点（相似度计算效率高，有效缓解数据稀疏，异质信息融合）</p>
<p> 知识表示学习主要方法：<br> 距离模型：SE<br> 单层神经网络模型：SLM<br> 能量模型：SME<br> 双线性模型：LFM，效果较好<br> 张量神经网络模型：NTN<br> 矩阵分解模型：与LFM思想类似，具有长处<br> 翻译模型：TransE，主流模型<br> 其他模型：全息表示模型</p>
<p> 主要挑战与已有解决方案：<br> 复杂关系建模：多个针对transE的改进方案<br> 多源信息融合：<br> 关系路径建模：多条路径推理</p>
<p> 未来研究方向展望：<br> 面向不同知识类型的知识表示学习<br> 多源信息融合的知识表示学习<br> 考虑复杂推理模式的知识表示学习<br> 其他研究方向：面向大规模知识库的在线学习和快速学习，知识分布式表示的应用</p>
<p> 启发idea：词向量为什么必须是同一维度的？复杂向量维度长，简单向量维度短。是否有一种度量方法可以有效度量不同维度文本之间的相似性-是否仍可以采用余弦相似度？</p>
<p> science-认知科学研究成果将知识类型分为四类：树状关系（生物分类系统），二维网格关系（地理位置），单维顺序关系（偏序关系），有向网络关系（关联或因果关系）<br> 事理图谱是否是专门面向第四种知识类型的知识库？</p>
</li>
<li><p>The Weltmodell: A Data-Driven Commonsense Knowledge Base，2014，短文,德国<br>大数据驱动的常识知识库</p>
<p>知识获取：</p>
<p>所使用数据集，事实抽取方法，互信息计算，相似度计算（concept，statement）,知识库构建结果。</p>
<p>展示：web</p>
<p>前景与展望：</p>
<p>改进事实抽取模块，集成外部知识库，增加可视化展示选项，加入用户反馈。</p>
<p>需要改进的：事实消歧，知识库补全，常识推理-不仅仅实现尝试推理，而且让人可以理解推理过程</p>
</li>
<li><p>A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books,2013,google</p>
<p>介绍了谷歌的公布的一个Syntactic-Ngrams语料库，这个语料库体积非常大，适合用来做各种nlp任务。</p>
</li>
<li><p>Verb Pattern: A Probabilistic Semantic Representation on Verbs，2016AAAI,复旦大学GBM lab，cuiwanyun</p>
<p>本工作针对英语动词的一词多义现象进行研究，提出了一种动词一词多义现象的表示方法：抽象出动词搭配的概念和特定搭配两种形式作为动词一词多义的表示。<br>动词：概念<br>动词：特定搭配<br>训练语料用的就是上面谷歌公布的Syntactic-Ngrams语料以及Probase知识库。</p>
</li>
<li><p>KRAKEN: N-ary Facts in Open Information Extraction,Weltmodell作者，NAACL，2012</p>
<p>提出了一个开放信息抽取系统，可以抽取多元知识，知识的完整性更好，准确率更高，每句话知识抽取率更高。<br>基于传统的分词，词性标注，句法依存分析，然后提出一个规则集合来抽取知识。   对噪声数据（语法不合规则）抽取质量不高，因为依存分析的准确率会下降。</p>
</li>
</ol>
<ol>
<li><p>Learning from the Web: Extracting General World Knowledge from Noisy Text，AAAI，2010</p>
<p>本工作对于构建事理图谱是一个很好的借鉴。</p>
<p>USA罗切斯特大学,从大规模网页博客和维基百科数据中抽取常识知识，是一个系列工作，从2001年到2012年一直在做的一个工作。<br>开发了一个英文常识知识抽取系统：Knext。<br>本文定义了常识知识的形式：</p>
<p>Information extraction efforts, e. g. Banko et al. (2007), have focused on learning facts about specific entities, such as that Alan Turing died in 1954 or that the capital of Bahrain is Manama. Knowledge bases of such facts are quite useful, but getting to human-level AI seems to depend less on this specific knowledge than it does on the most basic world knowledge – our commonsense un- derstanding of the world.</p>
<p>for the Wikipedia sentence “The emperor was succeeded by his son, Akihito”, what we seek to (and do) learn is that ‘AN EMPEROR MAY BE SUCCEED -ED BY A SON’ and ‘A MALE MAY HAVE A SON’ – not the specific information about Emperor Sho ̄wa and his son. Thus, hav- ing been written as a repository of information, which most weblogs are not, is not a clear advantage for Wikipedia as a resource for extracting background knowledge. </p>
</li>
</ol>
<pre><code>本文回到了四个问题：

    随着输入语料的不断增加，抽取到的知识量也在不断增加
    尽管博客和维基百科数据质量不同，抽取到的知识质量却大致相同
    对抽取到的原始知识进行规则过滤，可以大幅度提高知识质量
    从博客和维基百科抽取到的常识知识互相的覆盖率很小，尽管随着数据规模增加覆盖率也在不断缓慢增加

本文还提供了一个可借鉴的知识库评价方法：
手工评价，随机抽取一定量的知识，将每一条知识根据一定的标准评价为5个分值，然后计算平均得分来衡量知识的质量。

一个有用的考虑：对于超大规模的数据来说，知识抽取更加注重准确率，而不是recall。

最后一点：论文并没有说明所构建的知识库在实际AI系统当中的用处，所以我们应该注重事理图谱在工业中的应用价值，而不仅仅是想当然构建出一个玩物知识库。
</code></pre><ol>
<li><p>Learning Textual Graph Patterns to Detect Causal Event Relations,AAAI,2010</p>
<p>事件因果关系检测，提出了一种新的特征模板，利用SVM作为分类器，打败了前一个代表性工作的实验结果。</p>
<p>数据集已经标注好了两个事件之间是否有因果关系，且整个数据集只有1000条，对事理图谱构建帮助不大。</p>
</li>
<li><p>Mining Commonsense Knowledge From Personal Stories in Internet Weblogs，AKBC,2010</p>
</li>
<li><p>Commonsense Causal Reasoning Using Millions of Personal Stories，AAAI,2011   </p>
</li>
<li><p>Open-domain Commonsense Reasoning Using Discourse Relations from a Corpus of Weblog Stories，NAACL，2010</p>
</li>
<li><p>Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning，AAAI,2011 </p>
</li>
<li><p>Automatic Knowledge Base Construction using Probabilistic Extraction, Deductive Reasoning, and Human Feedback, 2012,AKBC</p>
</li>
<li><p>Automatic Construction of Inference-Supporting Knowledge Bases,AKBC,2014,best paper</p>
</li>
<li><p>KELVIN: a tool for automated knowledge base construction,NAACL,2013</p>
</li>
<li><p>Advances in Automated Knowledge Base Construction,2013,AKBC</p>
</li>
<li><p>Deep Learning for Event-Driven Stock Prediction，dingxiao,IJCAI,2015</p>
</li>
<li><p>Using Structured Events to Predict Stock Price Movement: An Empirical Investigation,dingxiao EMNLP,2014</p>
</li>
<li><p>Mining the Web to Predict Future Events,WSDM,2013,Kira Radinsky</p>
</li>
<li><p>Learning Causality for News Events Prediction,WWW,2012,Kira Radinsky</p>
<p>非常重要：</p>
<p>用因果模板构建因果关系对作为训练数据，构建因果事件图谱，实现事件泛化</p>
<p>用多种评价方法从多个角度对事件预测模型进行了评价。需要人工评价</p>
</li>
<li><p>因果关系及其在社会媒体上的应用研究综述,赵森栋，2014</p>
<p>因果分类：常识因果，浅层因果，深层因果</p>
<p>重要问题：学习词或短语语义之间的逻辑关系(尤其是 因果关系)进而表示成知识库的形式,与不断地优化因果抽取算法相比更有意义.</p>
<p>本质因果推断的三类方法：</p>
<pre><code>1.随机对照实验
2.准实验设计方法
3.联合模型方法：图模型和虚拟事实模型
</code></pre><p>问题与挑战：</p>
<pre><code>因果对识别：模板匹配和规则（常识因果）
因果模型的适用性-泛化能力
客观的评价指标
社会媒体上的因果分析-值得做
</code></pre><p>未来研究方向：社会媒体上的因果知识的识别和抽取，基于因果图模型的应用研究（贝叶斯网络）</p>
<pre><code>1.理论创新：
2.社会媒体上的应用：因果分析，基于因果知识的预测和推理
</code></pre></li>
<li><p>Event causality extraction based on connectives analysis，赵森栋，2015，neurocomputing</p>
<p>这是森栋师兄的第二个工作，从单个句子中抽取因果关系。第一步是识别句子内的因果单元，第二步确定两个单元之间有无因果关系，以及因果关系的方向。<br>创新之处在于1.引入了一个新的因果关联类别特征特征，2.提出了一个新的模型Hidden Naive Bayes model,可以识别特征之间的互相作用关系，降低过拟合。</p>
</li>
<li><p>事件常识的获取方法研究，王亚，曹存根，计算机科学，2015</p>
<p>事件分类，框架表示，事件常识知识获取，事件公理获取，与FrameNet的比较。<br>整个工作都是基于手工标注的，耗费大量人力物力。<br>需要一个自动事件挖掘系统来挖掘相应的事件常识。</p>
</li>
<li><p>微博知识图谱构建方法研究，杜亚军，2015，西华大学</p>
<p>本文是对知识图谱构建的一个很好的综述文章，相关研究描述的很清楚。尤其写明了知识图谱的评价方法。<br>[62],[63],[64],[65],[66],[67]<br>提出了微博知识图谱构建需要解决什么问题，但是并没有给出任何解决方案。但是本文提出的问题也是消费事理图谱构建的<br>面临的问题，所以十分具有借鉴意义。<br>概念对象：人物，事物，地点，事件，话题<br>概念、关系重复，一词多义，同义扩展，多义扩展，知识图谱的更新<br>分析：part3.4<br>应用：part3.5</p>
</li>
<li><p>基于层叠条件随机场的事件因果关系抽取*，PR &amp; AI,付 剑 锋</p>
<p>重要参考文献：[15]。<br>这篇文章的借鉴意义也很大，利用CRF建模事件之间的关系。可以作为消费事理图谱构建的一个可选方法。<br>需要标注语料，标注事件之间的关系。但是本文没有进行事件抽取，标注好事件以及因果关系，然后进行因果关系识别。<br>本文也定义了事件的概念。这里的事件只是一个事件触发词。</p>
</li>
<li><p>从大规模web语料中获取常识语料，朱耀，曹存根，2008</p>
</li>
</ol>
<ol>
<li><p>事件时序关系建模的研究与实现，张绍臣，2009 </p>
<p>文景转换：参考文献</p>
<p>国内外很多项目已经对文景转换进行了研究,AT&amp;T 实验室研究开发的<br>WordsEye 系统[1],能够根据简单的文本描述,通过对含有几千个三维图形<br>的模型库查找,生成静态的三维图像;隆德大学开发的 CarSim  系统能够处<br>理交通领域的真实事故报告,生成交通事故报告描述的动态场景[2],具有很<br>高的实用价值;我国中科院陆汝钤院士带领开发的“天鹅”系统[3]实现中文<br>故事到动画片,全自动辅助动画的自动生成。</p>
<p>几年来哈尔滨工业大学机器智能与翻译实验室在文景转换方面也取得了一些成果,尤其在三维空间物体摆放以及基于文本到动画的生成都有所收获[4]。</p>
<p>常识的定义和ConceptNet<br>事件的定义和相关研究：事件=表示事件的动词+动词的受事者。open,开，防晒<br>事件时序关系的分类和相关研究：TimeML 和 ACE 分别对时序关系进行了不同的分类。 </p>
<p>本研究使用 TimeBank 1.2 语料 182 篇文章。在本文的后续研究中使用到<br>Evita 来识别事件和 Classifier,BLinker 来识别和提取事件的时序关系。在<br>后边的章节中会对这几个工具进行进一步的说明。 </p>
<p>ACE 对时序关系的定义,主要集中在事件与时间表达式的关系,很少<br>涉及事件与事件之间的时序关系,而且多数关系相互区别度不高,主要依靠<br>是否存在明显的标记来区分不同的时序关系。 </p>
<p>综合 TimeML 和 ACE 对时序关系的定义,将区别度很小的不同时序关<br>系 进 行 归 类 , 得 到 本 课 题 定 义 的 3 类 时 序 关 系 : Before , After ,Simultaneous,分别表示两个事件之间发生的“前”,“后”和同时发生的时序关系。 </p>
</li>
</ol>
<pre><code>事件的识别（动词名词）和抽取（受事者抽取）：

事件合并：（完全相同的事件和相似事件，事件相似度计算方法）

待解决的问题：
中文处理
受事者提取
高频噪音事件节点删除
评测方法
</code></pre><ol>
<li><p>事件时序关系识别的研究与实现，孙辉，2010</p>
<p>本课题中中文研究现状以及分析部分特别有帮助，需要找到相关论文来看。</p>
<p>事件关系抽取是该项目的主要研究内容，而事件时序关系作为事件关系的<br>一种，旨在研究事件在时间上的先后顺序关系。本研究课题希望通过结合计算<br>语言学知识与统计机器学习方法，搭建事件时序关系机器自动识别平台。</p>
<p>本研究课题以英文为语言基础，以事件时序关系作为研究内容，将事件时<br>序关系知识库作为语义资源，以搭建事件时序关系识别模型作为研究目的。 </p>
</li>
</ol>
<pre><code>（1）应对事件时序关系识别模型的特征空间进一步细化。本研究只对每个
特征对模型的影响给出评价，并未考虑特征组合对模型的影响。 
（2）用统计机器学习方法搭建基于中文的事件时序关系识别模型。 
（3）本研究用 VerbNet 词汇资源搭建句法框架构造规则进行事件受事者识
别，效果不是很理想，可以考虑用基于统计机器学习的方法构造识别模型。

Evita(Events in text Analyzer)是 ARDA 资助的 TARSQI 研究框架下的事件识
别工具
[27]
。Evita 作为 TTK(Tarsqi Toolkit)工具包中的一个组件，它主要用来
进行事件识别和事件关联语法特征分且没有应用领域的限制，只需在事件标注
前用 AlembicWorkbench 标注器和 Chunker 工具对文本进行词性标注和 Chunk
预处理。Evita 的识别性能达到 80.12%的 F-measure。

如果不是动词性事件，直接断定事件受事者为空。 

从表 4-2 中可以看出，该模块的平均准确率为 57.00%。性能低下的原因在
于 VerbNet 句法框架的匹配率很低，Wiki 中编写者多用复杂句，而 VerbNet 中
的句法框架则是根据简单句建立起来的，这用情况下的事件受事者很难提取。

学者们一般将基于本体的语义相似度计算方法划分为 4 类[31]：基于距离的
语义相似度计算（Edge  Counting  Measures）、基于内容的语义相似度计算
（Information Content Measures）、基于属性点的语义相似度计算（Feature-based 
Measures）和混合式语义相似度计算（Hybrid Measures）。
</code></pre><ol>
<li><p>基于语义依存线索的事件关系识别方法研究，马彬 洪宇† 杨雪蓉 姚建民 朱巧明，2013</p>
<p>话题检测与跟踪，事件关系识别，具有参考价值<br>还没看完</p>
</li>
<li><p>BUEES: a Bottom-Up Event Extraction System,Xiao Ding</p>
<p>包含两个工作：<br>First：事件类型自动抽取。动词细分类。   Trigger抽取，Clustering，Filter。<br>Second：事件arguments抽取。</p>
</li>
<li><p>Building Chinese Event Type Paradigm Based on Trigger Clustering，Xiao Ding</p>
<p>本文工作包含在上一篇论文中，是BUEES的一个子工作。</p>
</li>
<li><p>ZORE: A Syntax-based System for Chinese Open Relation Extraction</p>
<p>关系抽取，弱动词结构的提取。<br>关系抽取的候选：三种类型。<br>语法语义模板匹配。<br>LR置信度结果过滤。<br>跟事件抽取本质是不一样的，事件抽取要有事件触发词。关系抽取关注两个NP之间的关系，最少要有两个NP结构。或者更多的NP结构则称为<br>N-ary facts。</p>
</li>
<li><p>Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!,IBM,EMNLP,2013</p>
<p>在学术界和工业界使用rule_based方法差异很大，主要在于两个社区的的评价指标存在巨大差异：方法是否耗时费力的定义有所不同，所耗费的软件硬件资源学术界一般不报告。作者很担心在大数据信息抽取时代这种gap会越来越扩大，最终对两个社区都没有好处，最后提出了3个设想来缓解这种gap。</p>
<p>最后作者提出的方法还没认真看。</p>
</li>
<li><p>A Domain-independent Rule-based Framework for Event Extraction，University of Arizona, Tucson, AZ, USA，<br>IJCNLP，2015</p>
<p>构建了一个领域独立的基于规则的开源事件抽取框架，有web界面，可视化匹配结果。<br>可以基于api编写抽取规则。</p>
</li>
<li><p>Storybase: Towards Building a Knowledge Base for News Events,The Pennsylvania State University<br>University Park, PA 16802, USA, ACL IJCNLP 2015</p>
<p>写作的参考论文。<br>这篇论文构建新闻事件的知识库，提供了检索界面。<br>本文结构可以作为很好的参考，还有图形图标的绘制。<br>参考价值较大。<br>related work列出的几篇参考文献值得看看。</p>
</li>
<li><p>MPQA 3.0: An Entity/Event-Level Sentiment Corpus,Lingjia Deng,Intelligent Systems Program , University of Pittsburgh,NAACL,2015 </p>
<p>MPQA是一个知识库，从实体，事件层次上来构建一个情感分析知识库。文本介绍了在MPQA2.0的基础上添加此信息的标注体系。<br>并分析了该知识库对于情感分析的作用。</p>
</li>
<li><p>Research on Event Prediction Algorithm Based on Event Sequence Semantic, 西北大学，2009,FSKD</p>
<p>该研究质量比较低</p>
</li>
<li><p>Liu Ting et al. - 2007 - Subdividing verbs to improve syntactic parsing,电子学报</p>
<p>最大熵模型实现动词细分类：8个类别。<br>然后细分类结果用于句法分析任务，发现动词细分类可以提高句法分析的性能。<br>2007年马金山，早期工作，需要找实验室车老师，或者哪位师兄问一问相关工作的实现。</p>
</li>
<li><p>The Annotation of Event Schema in Chinese，北语，杨尔宏老师。</p>
<p>中文事件标注体系。<br>参考价值不大。</p>
</li>
<li><p>Using Textual Patterns to Learn Expected Event Frequencies, Jonathan Gordon,Department of Computer Science University of Rochester Rochester,2012, AKBC </p>
<p>从文本中匹配获取常识事件发生的频率。</p>
</li>
</ol>
<ol>
<li><p>Web mining for event-based commonsense knowledge using lexico-syntactic pattern matching and semantic role labeling, Expert Systems with Applications journal,2010, taiwan</p>
<p>构造query，输入搜索引擎，处理返回的文本。<br>常识事件的定义和我们不同。<br>更像自动构造的concept net。<br>典型事件比如：Dog Eat Foods。</p>
</li>
<li><p>Ranking Multidocument Event Descriptions for Building Thematic Timelines, coling 2014,法国</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;深度学习相关论文&quot;&gt;&lt;a href=&quot;#深度学习相关论文&quot; class=&quot;headerlink&quot; title=&quot;深度学习相关论文&quot;&gt;&lt;/a&gt;深度学习相关论文&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;机器学习系统，隐藏多少技术债？Hidden Technical Debt in Machine Learning Systems&lt;/li&gt;
&lt;li&gt;Mastering the game of Go with deep neural networks and tree search_nature&lt;/li&gt;
&lt;li&gt;Human-level control through deep reinforcement&lt;/li&gt;
&lt;li&gt;Teaching Machines to Read and Comprehend&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="http://yoursite.com/2015/04/15/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2015/04/15/集成学习/</id>
    <published>2015-04-15T07:47:55.000Z</published>
    <updated>2016-11-06T06:24:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="看《a-course-in-ML》第十一章ensemble-learning所做的笔记"><a href="#看《a-course-in-ML》第十一章ensemble-learning所做的笔记" class="headerlink" title="看《a course in ML》第十一章ensemble learning所做的笔记:"></a>看《a course in ML》第十一章ensemble learning所做的笔记:</h3><p>In this chapter, you will learn about various ways of combining into ensembles. One of the shocking results we will see is that you can take a learning model that only ever does slightly better than chance, and turn it into an arbitrarily good learning model, through a technique known as boosting. You will also learn how ensembles can decrease the variance of predictors as well as perform regularization.</p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives:"></a>Learning Objectives:</h3><p>Implement bagging and explain how it reduces variance in a predictor.</p>
<p>实现bagging并解释为什么它可以降低方差<br><a id="more"></a></p>
<p>Explain the difference between a weak learner and a strong learner.</p>
<p>解释弱学习器和强学习器的不同</p>
<p>Derive the AdaBoost algorithm.</p>
<p>提出AdaBoost-adaptive boosting algorithm</p>
<p>Understand the relationship between boosting decision stumps and linear classification.</p>
<p>理解决策树桩的提升和线性分类器的关系</p>
<p>boosting方法的优点：</p>
<p>比随机猜测好一点点的弱分类器提升为一个很好的分类器</p>
<p>天生并行，训练和测试效率更高</p>
<p>降低方差，实施正则化的另一种方法</p>
<h3 id="11-1-多个分类器投票"><a href="#11-1-多个分类器投票" class="headerlink" title="11.1 多个分类器投票"></a>11.1 多个分类器投票</h3><p>训练很多基础分类器：DT,perceptron,KNN,NN of different architectures,多者胜出</p>
<p>不是所有分类器都会犯同一个错误，如果每一个错误只有少数分类器犯下，那么你就会得到优化的分类器</p>
<p>不幸的是不同的算法倾向于饭同一类错误</p>
<p>对降低方差有效</p>
<p>二分类容易扩展到多分类问题，但不容易扩展到一下问题，因为很难得到相同的输出：回归：均值；排序，聚类需要不同的方法</p>
<p>在一个相通数据集上训练或者平分原来的数据集都不是很好的方法</p>
<p>一个数据集获得多个数据集：bootstrap resampling，原理：独立同分布，bagging</p>
<p>一个样例不被选中的概率：1-1/N   在一个集合中不出现的概率1-1/N的N次方，1/e==0.3679    N=1000（集合样本数量）是已经精确到4位小数</p>
<p>所以约有63%的原始样本会出现在所有的集合中 </p>
<p>正则化-降低方差：通过使用超参数或bagging方法 </p>
<p>尽管每一个分类器都overfit，但是对不同的特征overfit，通过投票，可以去除掉很大一部分overfit</p>
<p>图形对比：错误率-DT depth   错误率-num of bags</p>
<h3 id="11-2-boosting-weak-learners"><a href="#11-2-boosting-weak-learners" class="headerlink" title="11.2 boosting weak learners"></a>11.2 boosting weak learners</h3><p>提升弱学习器：想起文件压缩-不断压缩直至1b</p>
<p>强学习算法定义-第十章：PAC (probably approximately correct)learning，以成功率y学习到一个算法,错误率必须降低到e</p>
<p>可以容易得到一个弱学习器：49%错误率，只要比随机猜测好就行</p>
<p>boosting相对来说是一种学习框架而不是一个算法</p>
<p>特定的boosting algorithm-AdaBoosting-adaptive boosting algorithm</p>
<p>第一个实用的boosting算法：多项式复杂度，不需定义很多的超参数，自动适应训练数据</p>
<p>与准备考试相比较，答对的问题降低注意力，答错的问题提高注意力（权重）</p>
<p>在训练数据上的权重分布d（初始化每个样本权重都是1/N），在这个有权重的数据集上训练弱学习器（假设可以接受加权样本数据，基本都是可以的），Fk的错误率用来决定这个函数的适应参数a，表示这个分类器结果的重要性。只要分类器的错误率小于50%，那么a就会大于零，如果错误率为0，那么这个分类器的适应参数就会无限大，相当于只需要这一个分类器的结果就够了。</p>
<p>a计算出来后，权重分布d在新一轮迭代中得到更新。分类正确的样本权重下降，分类错误的样本权重上升，Z是正则项，保证d的和为1。</p>
<p>算法最终返回结果：多个分类器的加权投票结果</p>
<p>一个具体的例子：对每个样本加权之后，如果要取得小于50%的错误率，就要有所更新。</p>
<p>shallow DT：with small depth</p>
<p>图形对比：一层的DT,多个boost  和  一个DT,深度逐渐增加的对比</p>
<p>decision stumps-AdaBoost起到了线性分类器的效果</p>
<p>一个具体的例子：<br>更进一步：最近的研究表明,AdaBoost算法优化目标为exponential loss</p>
<p>另一个例子：linear classifier-AdaBoost起到了两层神经网络的效果<br>K个线性分类器，K个隐藏单元，<br>仅仅是结构相同-但是参数学习方法不同，一旦学习到了线性分类器的W和b参数，就再也不会更新了。</p>
<h3 id="11-3-random-ensembles随机集成算法"><a href="#11-3-random-ensembles随机集成算法" class="headerlink" title="11.3 random ensembles随机集成算法"></a>11.3 random ensembles随机集成算法</h3><p>选择决策树的结构非常耗时，树桩算法很高效</p>
<p>固定树的结构，随机选择特征（一个特征甚至可以在一个分支当中多次出现）：随机森林<br>D（数据），depth（树深），K（决策树的数量）</p>
<p>最终分类器：多个随机树的投票</p>
<p>这种方法效果很好：不同特征之间不相关时效果最好，因为每一棵树可能用到的特征很少。一些树会query无用的特征，产生随机猜测结果，可以作为噪声数据，最终只有好的决策树有效。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;看《a-course-in-ML》第十一章ensemble-learning所做的笔记&quot;&gt;&lt;a href=&quot;#看《a-course-in-ML》第十一章ensemble-learning所做的笔记&quot; class=&quot;headerlink&quot; title=&quot;看《a course in ML》第十一章ensemble learning所做的笔记:&quot;&gt;&lt;/a&gt;看《a course in ML》第十一章ensemble learning所做的笔记:&lt;/h3&gt;&lt;p&gt;In this chapter, you will learn about various ways of combining into ensembles. One of the shocking results we will see is that you can take a learning model that only ever does slightly better than chance, and turn it into an arbitrarily good learning model, through a technique known as boosting. You will also learn how ensembles can decrease the variance of predictors as well as perform regularization.&lt;/p&gt;
&lt;h3 id=&quot;Learning-Objectives&quot;&gt;&lt;a href=&quot;#Learning-Objectives&quot; class=&quot;headerlink&quot; title=&quot;Learning Objectives:&quot;&gt;&lt;/a&gt;Learning Objectives:&lt;/h3&gt;&lt;p&gt;Implement bagging and explain how it reduces variance in a predictor.&lt;/p&gt;
&lt;p&gt;实现bagging并解释为什么它可以降低方差&lt;br&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>shell基本编程自己用过的一些经典例子</title>
    <link href="http://yoursite.com/2015/04/15/shell%E8%84%9A%E6%9C%AC/"/>
    <id>http://yoursite.com/2015/04/15/shell脚本/</id>
    <published>2015-04-15T05:37:48.000Z</published>
    <updated>2016-11-07T08:49:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>文本行中各列打乱：<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk <span class="symbol">'BEGIN</span>&#123;srand()&#125;&#123;<span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=NF;i++) b[rand()NF]=$i&#125;<span class="keyword">END</span>&#123;<span class="keyword">for</span>(x <span class="keyword">in</span> b)printf <span class="string">"%s "</span>,b[x]&#125;' data</div></pre></td></tr></table></figure></p>
<p>文本行中各行打乱：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk '<span class="keyword">BEGIN</span>&#123;srand()&#125;&#123;b[<span class="keyword">rand</span>()NR]=$<span class="number">0</span>&#125;<span class="keyword">END</span>&#123;<span class="keyword">for</span>(x <span class="keyword">in</span> b)print b[x]&#125;<span class="string">' data</span></div></pre></td></tr></table></figure></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">shuf</span> -n1000 <span class="class"><span class="keyword">data</span></span></div></pre></td></tr></table></figure>
<a id="more"></a>
<p>以a.txt作为pattern查找b.txt，实际是求2者交集：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -F -x -f <span class="selector-tag">a</span><span class="selector-class">.txt</span> <span class="selector-tag">b</span>.txt</div></pre></td></tr></table></figure></p>
<p>以a.txt作为pattern查找b.txt，显示不在a.txt中的数据，实际是求差集b-a：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -F -v -x -f <span class="selector-tag">a</span><span class="selector-class">.txt</span> <span class="selector-tag">b</span>.txt</div></pre></td></tr></table></figure></p>
<p>编码转换：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iconv <span class="_">-f</span> gb18030 -t utf8 filename</div></pre></td></tr></table></figure></p>
<p>以_为分隔符，第二列为键值排序，稳定排序（默认不稳定）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sort -t_ -k1,2  <span class="_">-s</span>  filename</div></pre></td></tr></table></figure></p>
<p>对大文件进行外部排序并去重，以temp目录作为缓存：<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">sort -u -T temp filename</span></div></pre></td></tr></table></figure></p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">sort</span> filename | uniq   -<span class="built_in">c</span>计数唯一的排序的记录 -d仅仅显示重复的记录 -u仅仅显示没有重复的记录</div></pre></td></tr></table></figure>
<p>在preview中打开awk的man文档：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">man</span> -t awk | <span class="keyword">open</span> -a Preview -<span class="built_in">f</span></div></pre></td></tr></table></figure></p>
<p>awk和cut简单用法：<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">awk</span> -F: -v <span class="string">'OFS=\t'</span> <span class="string">'&#123;print "all users are:",<span class="variable">$1</span>,<span class="variable">$2</span>,<span class="variable">$3</span>,<span class="variable">$4</span>,<span class="variable">$5</span>,<span class="variable">$6</span>,<span class="variable">$7</span>&#125;'</span> /etc/passwd | cut -f <span class="number">1</span>,<span class="number">6</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wc <span class="_">-l</span> 行数 -c字节数 -w字数</div></pre></td></tr></table></figure>
<p>查看端口:<br><figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">lsof</span> -i tcp:<span class="keyword">port</span> 将<span class="keyword">port</span>换成被占用的端口(如：8086、9998)</div></pre></td></tr></table></figure></p>
<p>查看自己的进程详细情况:<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">ps</span> -ef |<span class="keyword">grep</span> zyli</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本行中各列打乱：&lt;br&gt;&lt;figure class=&quot;highlight ada&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;awk &lt;span class=&quot;symbol&quot;&gt;&#39;BEGIN&lt;/span&gt;&amp;#123;srand()&amp;#125;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(i=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;;i&amp;lt;=NF;i++) b[rand()NF]=$i&amp;#125;&lt;span class=&quot;keyword&quot;&gt;END&lt;/span&gt;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(x &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; b)printf &lt;span class=&quot;string&quot;&gt;&quot;%s &quot;&lt;/span&gt;,b[x]&amp;#125;&#39; data&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;文本行中各行打乱：&lt;br&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;awk &#39;&lt;span class=&quot;keyword&quot;&gt;BEGIN&lt;/span&gt;&amp;#123;srand()&amp;#125;&amp;#123;b[&lt;span class=&quot;keyword&quot;&gt;rand&lt;/span&gt;()NR]=$&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;&lt;span class=&quot;keyword&quot;&gt;END&lt;/span&gt;&amp;#123;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(x &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; b)print b[x]&amp;#125;&lt;span class=&quot;string&quot;&gt;&#39; data&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight haskell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;title&quot;&gt;shuf&lt;/span&gt; -n1000 &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;data&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="shell" scheme="http://yoursite.com/tags/shell/"/>
    
  </entry>
  
</feed>
